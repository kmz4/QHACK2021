<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>qose.train_utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>qose.train_utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import pennylane as qml
from pennylane import numpy as np
import autograd.numpy as np
from autograd.numpy import exp
import itertools
import time


def hinge_loss(labels, predictions, type=&#39;L2&#39;):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 
      type:  (Default value = &#39;L2&#39;)

    Returns:

    &#34;&#34;&#34;
    loss = 0
    for l, p in zip(labels, predictions):
        if type == &#39;L1&#39;:
            loss = loss + np.abs(l - p)  # L1 loss
        elif type == &#39;L2&#39;:
            loss = loss + (l - p) ** 2  # L2 loss
    loss = loss / labels.shape[0]
    return loss


def ohe_accuracy(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;
    loss = 0
    for l, p in zip(labels, predictions):
        loss += np.argmax(l) == np.argmax(p)
    return loss / labels.shape[0]

def wn_accuracy(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;

    loss = 0
    #tol = 0.05
    tol = 0.1
    for l, p in zip(labels, predictions):
        if abs(l - p) &lt; tol:
            loss = loss + 1
    loss = loss / labels.shape[0]

    return loss

def mse(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;
    # print(labels.shape, predictions.shape)
    loss = 0
    for l, p in zip(labels, predictions):
        loss += np.sum((l - p) ** 2)
    return loss / labels.shape[0]

def make_predictions(circuit,pre_trained_vals,X,Y,**kwargs):
    &#34;&#34;&#34;

    Args:
      circuit: 
      pre_trained_vals: 
      X: 
      Y: 
      **kwargs: 

    Returns:

    &#34;&#34;&#34;

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = pre_trained_vals

    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = pre_trained_vals

    # make final predictions
    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        final_predictions = np.stack([circuit(var, x) for x in X])
        acc=ohe_accuracy(Y,predictions)

    elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
        from autograd.numpy import exp
        n = kwargs.get(&#39;nqubits&#39;)
        w = var[:,-1]
        theta = var[:,:-1].numpy()
        final_predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,1)) for x in X]
        acc=wn_accuracy(Y,predictions)

    return final_predictions,acc

def train_circuit(circuit, parameter_shape,X_train, Y_train, batch_size, learning_rate,**kwargs):
    &#34;&#34;&#34;train a circuit classifier

    Args:
      circuit(qml.QNode): A circuit that you want to train
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
    the second one is the number of layers in the circuit architecture.
      X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
      Y_train(np.ndarray): An array of size (M,) which are the categorical labels
    associated to the training data.
      batch_size(int): Batch size for the circuit training.
      learning_rate(float): The learning rate/step size of the optimizer.
      kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
    nsteps (int) : Number of training steps.
    optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
    Pass as qml.OptimizerName.
    Tmax (list): Maximum point T as defined in https://arxiv.org/abs/2010.08512. (Definition 8)
    The first element is the maximum number of parameters among all architectures,
    the second is the maximum inference time among all architectures in terms of computing time,
    the third one is the maximum inference time among all architectures in terms of the number of CNOTS
    in the circuit
    rate_type (string): Determines the type of error rate in the W-coefficient.
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit
    is equal to the time it takes to evaluate the accuracy of the trained circuit with
    respect to a validation batch three times the size of the training batch size and
    the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit is equal to the time
    it takes to train the circuit (for nsteps training steps) and compute the cost at
    each step and the error rate is equal to the cost after nsteps training steps.
      **kwargs: 

    Returns:
      W_: W-coefficient, trained weights

    &#34;&#34;&#34;
    #print(&#39;batch_size&#39;,batch_size)
    # fix the seed while debugging
    #np.random.seed(1337)
    def ohe_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        predictions = (np.stack([circuit(params, x) for x in ang_array]) + 1) * 0.5
        return mse(actual, predictions)

    def wn_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        w = params[:,-1]

        theta = params[:,:-1]
        #print(w.shape,w,theta.shape,theta)
        predictions = np.asarray([2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1. for x in ang_array])
        return mse(actual, predictions)

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = np.zeros(parameter_shape)
    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = np.hstack((np.zeros(parameter_shape),np.random.random((kwargs[&#39;nqubits&#39;],1))-0.5))
    rate_type = kwargs[&#39;rate_type&#39;]
    inf_time = kwargs[&#39;inf_time&#39;]
    optim = kwargs[&#39;optim&#39;]
    numcnots = kwargs[&#39;numcnots&#39;]

    Tmax = kwargs[&#39;Tmax&#39;] #Tmax[0] is maximum parameter size, Tmax[1] maximum inftime (timeit),Tmax[2] maximum number of entangling gates
    num_train = len(Y_train)
    validation_size = int(0.1*num_train)
    opt = optim(stepsize=learning_rate) #all optimizers in autograd module take in argument stepsize, so this works for all
    start = time.time()
    for _ in range(kwargs[&#39;nsteps&#39;]):
        batch_index = np.random.randint(0, num_train, (batch_size,))
        X_train_batch = np.asarray(X_train[batch_index])
        Y_train_batch = np.asarray(Y_train[batch_index])
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            var, cost = opt.step_and_cost(lambda v: ohe_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            var, cost = opt.step_and_cost(lambda v: wn_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
    end = time.time()
    cost_time = (end - start)

    if kwargs[&#39;rate_type&#39;] == &#39;accuracy&#39;:
        validation_batch = np.random.randint(0, num_train, (validation_size,))
        X_validation_batch = np.asarray(X_train[validation_batch])
        Y_validation_batch = np.asarray(Y_train[validation_batch])
        start = time.time()  # add in timeit function from Wbranch
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            predictions = np.stack([circuit(var, x) for x in X_validation_batch])
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            n = kwargs.get(&#39;nqubits&#39;)
            w = var[:,-1]
            theta = var[:,:-1]
            predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,1)) for x in X_validation_batch]
        end = time.time()
        inftime = (end - start) / len(X_validation_batch)
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            err_rate = (1.0 - ohe_accuracy(Y_validation_batch,predictions))+10**-7 #add small epsilon to prevent divide by 0 errors
            #print(&#39;error rate:&#39;,err_rate)
            #print(&#39;weights: &#39;,var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            err_rate = (1.0 - wn_accuracy(Y_validation_batch,predictions))+10**-7 #add small epsilon to prevent divide by 0 errors
            #print(&#39;error rate:&#39;,err_rate)
            #print(&#39;weights: &#39;,var)
    elif kwargs[&#39;rate_type&#39;] == &#39;batch_cost&#39;:
        err_rate = (cost) + 10**-7 #add small epsilon to prevent divide by 0 errors
        #print(&#39;error rate:&#39;,err_rate)
        #print(&#39;weights: &#39;,var)
        inftime = cost_time
    # QHACK #

    if kwargs[&#39;inf_time&#39;] ==&#39;timeit&#39;:

        W_ = np.abs((Tmax[0] - len(var)) / (Tmax[0])) * np.abs((Tmax[1] - inftime) / (Tmax[1])) * (1. / err_rate)

    elif kwargs[&#39;inf_time&#39;]==&#39;numcnots&#39;:
        nc_ = numcnots
        W_ = np.abs((Tmax[0] - len(var)) / (Tmax[0])) * np.abs((Tmax[2] - nc_) / (Tmax[2])) * (1. / err_rate)

    return W_,var

def evaluate_w(circuit, n_params, X_train, Y_train, **kwargs):
    &#34;&#34;&#34;together with the function train_circuit(...) this executes lines 7-8 in the Algorithm 1 pseudo code of (de Wynter 2020)
    batch_sets and learning_rates are lists, if just single values needed then pass length-1 lists

    Args:
      circuit: 
      n_params: 
      X_train: 
      Y_train: 
      **kwargs: 

    Returns:

    &#34;&#34;&#34;
    Wmax = 0.0
    batch_sets = kwargs.get(&#39;batch_sizes&#39;)
    learning_rates=kwargs.get(&#39;learning_rates&#39;)
    hyperparameter_space = list(itertools.product(batch_sets, learning_rates))
    for idx, sdx in hyperparameter_space:
        wtemp, weights = train_circuit(circuit, n_params,X_train, Y_train, batch_size=idx, learning_rate=sdx, **kwargs)
        if wtemp &gt;= Wmax:
            Wmax = wtemp
            saved_weights = weights
    return Wmax, saved_weights

def train_best(circuit, pre_trained_vals,X_train, Y_train, batch_size, learning_rate,**kwargs):
    &#34;&#34;&#34;train a circuit classifier

    Args:
      circuit(qml.QNode): A circuit that you want to train
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
    the second one is the number of layers in the circuit architecture.
      X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
      Y_train(np.ndarray): An array of size (M,) which are the categorical labels
    associated to the training data.
      batch_size(int): Batch size for the circuit training.
      learning_rate(float): The learning rate/step size of the optimizer.
      kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
    nsteps (int) : Number of training steps.
    optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
    Pass as qml.OptimizerName.
    Tmax (list): Maximum point T as defined in https://arxiv.org/abs/2010.08512. (Definition 8)
    The first element is the maximum number of parameters among all architectures,
    the second is the maximum inference time among all architectures in terms of computing time,
    the third one is the maximum inference time among all architectures in terms of the number of CNOTS
    in the circuit
    rate_type (string): Determines the type of error rate in the W-coefficient.
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit
    is equal to the time it takes to evaluate the accuracy of the trained circuit with
    respect to a validation batch three times the size of the training batch size and
    the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit is equal to the time
    it takes to train the circuit (for nsteps training steps) and compute the cost at
    each step and the error rate is equal to the cost after nsteps training steps.
      pre_trained_vals: 
      **kwargs: 

    Returns:
      Yprime: final predictions, final accuracy

    &#34;&#34;&#34;
    from autograd.numpy import exp
    def ohe_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        predictions = (np.stack([circuit(params, x) for x in ang_array]) + 1) * 0.5
        return mse(actual, predictions)

    def wn_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        w = params[:,-1]

        theta = params[:,:-1]
        print(w.shape,w,theta.shape,theta)
        predictions = np.asarray([2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta,x)))))- 1. for x in ang_array])
        return mse(actual, predictions)

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = pre_trained_vals
    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = pre_trained_vals
    rate_type = kwargs[&#39;rate_type&#39;]
    optim = kwargs[&#39;optim&#39;]
    num_train = len(Y_train)
    validation_size = int(0.1*num_train)
    opt = optim(stepsize=learning_rate) #all optimizers in autograd module take in argument stepsize, so this works for all

    for _ in range(kwargs[&#39;nsteps&#39;]):
        batch_index = np.random.randint(0, num_train, (batch_size,))
        X_train_batch = np.asarray(X_train[batch_index])
        Y_train_batch = np.asarray(Y_train[batch_index])

        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            var, cost = opt.step_and_cost(lambda v: ohe_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            print(var)
            var, cost = opt.step_and_cost(lambda v: wn_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        print(_,cost)
        # check for early stopping
        if _%5==0:
            validation_batch = np.random.randint(0, num_train, (validation_size,))
            X_validation_batch = np.asarray(X_train[validation_batch])
            Y_validation_batch = np.asarray(Y_train[validation_batch])
            if kwargs[&#39;rate_type&#39;] == &#39;accuracy&#39;:
                if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
                    predictions = np.stack([circuit(var, x) for x in X_validation_batch])
                    acc=ohe_accuracy(Y_validation_batch,predictions)
                elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
                    n = kwargs.get(&#39;nqubits&#39;)
                    w = var[:,-1]
                    theta = var[:,:-1].numpy()
                    predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, x)))))- 1.,1)) for x in X_validation_batch]
                    acc=wn_accuracy(Y_validation_batch,predictions)
                if acc&gt;0.95:
                    break

            elif kwargs[&#39;rate_type&#39;] == &#39;batch_cost&#39;:
                if cost &lt; 0.001:
                    break
    # make final predictions
    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        final_predictions = np.stack([circuit(var, x) for x in X_train])
    elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
        n = kwargs.get(&#39;nqubits&#39;)
        w = var[:,-1]
        theta = var[:,:-1]
        final_predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, x)))))- 1.,1)) for x in X_train]
    return var,final_predictions</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="qose.train_utils.evaluate_w"><code class="name flex">
<span>def <span class="ident">evaluate_w</span></span>(<span>circuit, n_params, X_train, Y_train, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>together with the function train_circuit(&hellip;) this executes lines 7-8 in the Algorithm 1 pseudo code of (de Wynter 2020)
batch_sets and learning_rates are lists, if just single values needed then pass length-1 lists</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>circuit</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>n_params</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>X_train</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Y_train</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def evaluate_w(circuit, n_params, X_train, Y_train, **kwargs):
    &#34;&#34;&#34;together with the function train_circuit(...) this executes lines 7-8 in the Algorithm 1 pseudo code of (de Wynter 2020)
    batch_sets and learning_rates are lists, if just single values needed then pass length-1 lists

    Args:
      circuit: 
      n_params: 
      X_train: 
      Y_train: 
      **kwargs: 

    Returns:

    &#34;&#34;&#34;
    Wmax = 0.0
    batch_sets = kwargs.get(&#39;batch_sizes&#39;)
    learning_rates=kwargs.get(&#39;learning_rates&#39;)
    hyperparameter_space = list(itertools.product(batch_sets, learning_rates))
    for idx, sdx in hyperparameter_space:
        wtemp, weights = train_circuit(circuit, n_params,X_train, Y_train, batch_size=idx, learning_rate=sdx, **kwargs)
        if wtemp &gt;= Wmax:
            Wmax = wtemp
            saved_weights = weights
    return Wmax, saved_weights</code></pre>
</details>
</dd>
<dt id="qose.train_utils.hinge_loss"><code class="name flex">
<span>def <span class="ident">hinge_loss</span></span>(<span>labels, predictions, type='L2')</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>predictions</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>type</code></strong></dt>
<dd>(Default value = 'L2')</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hinge_loss(labels, predictions, type=&#39;L2&#39;):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 
      type:  (Default value = &#39;L2&#39;)

    Returns:

    &#34;&#34;&#34;
    loss = 0
    for l, p in zip(labels, predictions):
        if type == &#39;L1&#39;:
            loss = loss + np.abs(l - p)  # L1 loss
        elif type == &#39;L2&#39;:
            loss = loss + (l - p) ** 2  # L2 loss
    loss = loss / labels.shape[0]
    return loss</code></pre>
</details>
</dd>
<dt id="qose.train_utils.make_predictions"><code class="name flex">
<span>def <span class="ident">make_predictions</span></span>(<span>circuit, pre_trained_vals, X, Y, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>circuit</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>pre_trained_vals</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>X</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>Y</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def make_predictions(circuit,pre_trained_vals,X,Y,**kwargs):
    &#34;&#34;&#34;

    Args:
      circuit: 
      pre_trained_vals: 
      X: 
      Y: 
      **kwargs: 

    Returns:

    &#34;&#34;&#34;

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = pre_trained_vals

    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = pre_trained_vals

    # make final predictions
    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        final_predictions = np.stack([circuit(var, x) for x in X])
        acc=ohe_accuracy(Y,predictions)

    elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
        from autograd.numpy import exp
        n = kwargs.get(&#39;nqubits&#39;)
        w = var[:,-1]
        theta = var[:,:-1].numpy()
        final_predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,1)) for x in X]
        acc=wn_accuracy(Y,predictions)

    return final_predictions,acc</code></pre>
</details>
</dd>
<dt id="qose.train_utils.mse"><code class="name flex">
<span>def <span class="ident">mse</span></span>(<span>labels, predictions)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>predictions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mse(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;
    # print(labels.shape, predictions.shape)
    loss = 0
    for l, p in zip(labels, predictions):
        loss += np.sum((l - p) ** 2)
    return loss / labels.shape[0]</code></pre>
</details>
</dd>
<dt id="qose.train_utils.ohe_accuracy"><code class="name flex">
<span>def <span class="ident">ohe_accuracy</span></span>(<span>labels, predictions)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>predictions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ohe_accuracy(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;
    loss = 0
    for l, p in zip(labels, predictions):
        loss += np.argmax(l) == np.argmax(p)
    return loss / labels.shape[0]</code></pre>
</details>
</dd>
<dt id="qose.train_utils.train_best"><code class="name flex">
<span>def <span class="ident">train_best</span></span>(<span>circuit, pre_trained_vals, X_train, Y_train, batch_size, learning_rate, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>train a circuit classifier</p>
<h2 id="args">Args</h2>
<dl>
<dt>circuit(qml.QNode): A circuit that you want to train</dt>
<dt><strong><code>parameter_shape</code></strong></dt>
<dd>A tuple describing the shape of the parameters. The first entry is the number of qubits,</dd>
<dt><strong><code>parameter_shape</code></strong></dt>
<dd>A tuple describing the shape of the parameters. The first entry is the number of qubits,</dd>
</dl>
<p>the second one is the number of layers in the circuit architecture.
X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
Y_train(np.ndarray): An array of size (M,) which are the categorical labels
associated to the training data.
batch_size(int): Batch size for the circuit training.
learning_rate(float): The learning rate/step size of the optimizer.
kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
nsteps (int) : Number of training steps.
optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
Pass as qml.OptimizerName.
Tmax (list): Maximum point T as defined in <a href="https://arxiv.org/abs/2010.08512.">https://arxiv.org/abs/2010.08512.</a> (Definition 8)
The first element is the maximum number of parameters among all architectures,
the second is the maximum inference time among all architectures in terms of computing time,
the third one is the maximum inference time among all architectures in terms of the number of CNOTS
in the circuit
rate_type (string): Determines the type of error rate in the W-coefficient.
If rate_type == 'accuracy', the inference time of the circuit
is equal to the time it takes to evaluate the accuracy of the trained circuit with
respect to a validation batch three times the size of the training batch size and
the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
If rate_type == 'accuracy', the inference time of the circuit is equal to the time
it takes to train the circuit (for nsteps training steps) and compute the cost at
each step and the error rate is equal to the cost after nsteps training steps.
pre_trained_vals:
**kwargs: </p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Yprime</code></dt>
<dd>final predictions, final accuracy</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_best(circuit, pre_trained_vals,X_train, Y_train, batch_size, learning_rate,**kwargs):
    &#34;&#34;&#34;train a circuit classifier

    Args:
      circuit(qml.QNode): A circuit that you want to train
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
    the second one is the number of layers in the circuit architecture.
      X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
      Y_train(np.ndarray): An array of size (M,) which are the categorical labels
    associated to the training data.
      batch_size(int): Batch size for the circuit training.
      learning_rate(float): The learning rate/step size of the optimizer.
      kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
    nsteps (int) : Number of training steps.
    optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
    Pass as qml.OptimizerName.
    Tmax (list): Maximum point T as defined in https://arxiv.org/abs/2010.08512. (Definition 8)
    The first element is the maximum number of parameters among all architectures,
    the second is the maximum inference time among all architectures in terms of computing time,
    the third one is the maximum inference time among all architectures in terms of the number of CNOTS
    in the circuit
    rate_type (string): Determines the type of error rate in the W-coefficient.
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit
    is equal to the time it takes to evaluate the accuracy of the trained circuit with
    respect to a validation batch three times the size of the training batch size and
    the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit is equal to the time
    it takes to train the circuit (for nsteps training steps) and compute the cost at
    each step and the error rate is equal to the cost after nsteps training steps.
      pre_trained_vals: 
      **kwargs: 

    Returns:
      Yprime: final predictions, final accuracy

    &#34;&#34;&#34;
    from autograd.numpy import exp
    def ohe_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        predictions = (np.stack([circuit(params, x) for x in ang_array]) + 1) * 0.5
        return mse(actual, predictions)

    def wn_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        w = params[:,-1]

        theta = params[:,:-1]
        print(w.shape,w,theta.shape,theta)
        predictions = np.asarray([2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta,x)))))- 1. for x in ang_array])
        return mse(actual, predictions)

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = pre_trained_vals
    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = pre_trained_vals
    rate_type = kwargs[&#39;rate_type&#39;]
    optim = kwargs[&#39;optim&#39;]
    num_train = len(Y_train)
    validation_size = int(0.1*num_train)
    opt = optim(stepsize=learning_rate) #all optimizers in autograd module take in argument stepsize, so this works for all

    for _ in range(kwargs[&#39;nsteps&#39;]):
        batch_index = np.random.randint(0, num_train, (batch_size,))
        X_train_batch = np.asarray(X_train[batch_index])
        Y_train_batch = np.asarray(Y_train[batch_index])

        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            var, cost = opt.step_and_cost(lambda v: ohe_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            print(var)
            var, cost = opt.step_and_cost(lambda v: wn_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        print(_,cost)
        # check for early stopping
        if _%5==0:
            validation_batch = np.random.randint(0, num_train, (validation_size,))
            X_validation_batch = np.asarray(X_train[validation_batch])
            Y_validation_batch = np.asarray(Y_train[validation_batch])
            if kwargs[&#39;rate_type&#39;] == &#39;accuracy&#39;:
                if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
                    predictions = np.stack([circuit(var, x) for x in X_validation_batch])
                    acc=ohe_accuracy(Y_validation_batch,predictions)
                elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
                    n = kwargs.get(&#39;nqubits&#39;)
                    w = var[:,-1]
                    theta = var[:,:-1].numpy()
                    predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, x)))))- 1.,1)) for x in X_validation_batch]
                    acc=wn_accuracy(Y_validation_batch,predictions)
                if acc&gt;0.95:
                    break

            elif kwargs[&#39;rate_type&#39;] == &#39;batch_cost&#39;:
                if cost &lt; 0.001:
                    break
    # make final predictions
    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        final_predictions = np.stack([circuit(var, x) for x in X_train])
    elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
        n = kwargs.get(&#39;nqubits&#39;)
        w = var[:,-1]
        theta = var[:,:-1]
        final_predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, x)))))- 1.,1)) for x in X_train]
    return var,final_predictions</code></pre>
</details>
</dd>
<dt id="qose.train_utils.train_circuit"><code class="name flex">
<span>def <span class="ident">train_circuit</span></span>(<span>circuit, parameter_shape, X_train, Y_train, batch_size, learning_rate, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>train a circuit classifier</p>
<h2 id="args">Args</h2>
<dl>
<dt>circuit(qml.QNode): A circuit that you want to train</dt>
<dt><strong><code>parameter_shape</code></strong></dt>
<dd>A tuple describing the shape of the parameters. The first entry is the number of qubits,</dd>
<dt><strong><code>parameter_shape</code></strong></dt>
<dd>A tuple describing the shape of the parameters. The first entry is the number of qubits,</dd>
</dl>
<p>the second one is the number of layers in the circuit architecture.
X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
Y_train(np.ndarray): An array of size (M,) which are the categorical labels
associated to the training data.
batch_size(int): Batch size for the circuit training.
learning_rate(float): The learning rate/step size of the optimizer.
kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
nsteps (int) : Number of training steps.
optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
Pass as qml.OptimizerName.
Tmax (list): Maximum point T as defined in <a href="https://arxiv.org/abs/2010.08512.">https://arxiv.org/abs/2010.08512.</a> (Definition 8)
The first element is the maximum number of parameters among all architectures,
the second is the maximum inference time among all architectures in terms of computing time,
the third one is the maximum inference time among all architectures in terms of the number of CNOTS
in the circuit
rate_type (string): Determines the type of error rate in the W-coefficient.
If rate_type == 'accuracy', the inference time of the circuit
is equal to the time it takes to evaluate the accuracy of the trained circuit with
respect to a validation batch three times the size of the training batch size and
the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
If rate_type == 'accuracy', the inference time of the circuit is equal to the time
it takes to train the circuit (for nsteps training steps) and compute the cost at
each step and the error rate is equal to the cost after nsteps training steps.
**kwargs: </p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>W_</code></dt>
<dd>W-coefficient, trained weights</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_circuit(circuit, parameter_shape,X_train, Y_train, batch_size, learning_rate,**kwargs):
    &#34;&#34;&#34;train a circuit classifier

    Args:
      circuit(qml.QNode): A circuit that you want to train
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
      parameter_shape: A tuple describing the shape of the parameters. The first entry is the number of qubits,
    the second one is the number of layers in the circuit architecture.
      X_train(np.ndarray): An array of floats of size (M, n) to be used as training data.
      Y_train(np.ndarray): An array of size (M,) which are the categorical labels
    associated to the training data.
      batch_size(int): Batch size for the circuit training.
      learning_rate(float): The learning rate/step size of the optimizer.
      kwargs: Hyperparameters for the training (passed as keyword arguments). There are the following hyperparameters:
    nsteps (int) : Number of training steps.
    optim (pennylane.optimize instance): Optimizer used during the training of the circuit.
    Pass as qml.OptimizerName.
    Tmax (list): Maximum point T as defined in https://arxiv.org/abs/2010.08512. (Definition 8)
    The first element is the maximum number of parameters among all architectures,
    the second is the maximum inference time among all architectures in terms of computing time,
    the third one is the maximum inference time among all architectures in terms of the number of CNOTS
    in the circuit
    rate_type (string): Determines the type of error rate in the W-coefficient.
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit
    is equal to the time it takes to evaluate the accuracy of the trained circuit with
    respect to a validation batch three times the size of the training batch size and
    the error rate is equal to 1-accuracy (w.r.t. to a validation batch).
    If rate_type == &#39;accuracy&#39;, the inference time of the circuit is equal to the time
    it takes to train the circuit (for nsteps training steps) and compute the cost at
    each step and the error rate is equal to the cost after nsteps training steps.
      **kwargs: 

    Returns:
      W_: W-coefficient, trained weights

    &#34;&#34;&#34;
    #print(&#39;batch_size&#39;,batch_size)
    # fix the seed while debugging
    #np.random.seed(1337)
    def ohe_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        predictions = (np.stack([circuit(params, x) for x in ang_array]) + 1) * 0.5
        return mse(actual, predictions)

    def wn_cost_fcn(params, circuit, ang_array, actual):
        &#34;&#34;&#34;use MAE to start

        Args:
          params: 
          circuit: 
          ang_array: 
          actual: 

        Returns:

        &#34;&#34;&#34;
        w = params[:,-1]

        theta = params[:,:-1]
        #print(w.shape,w,theta.shape,theta)
        predictions = np.asarray([2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1. for x in ang_array])
        return mse(actual, predictions)

    if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
        var = np.zeros(parameter_shape)
    elif kwargs[&#39;readout_layer&#39;]==&#34;weighted_neuron&#34;:
        var = np.hstack((np.zeros(parameter_shape),np.random.random((kwargs[&#39;nqubits&#39;],1))-0.5))
    rate_type = kwargs[&#39;rate_type&#39;]
    inf_time = kwargs[&#39;inf_time&#39;]
    optim = kwargs[&#39;optim&#39;]
    numcnots = kwargs[&#39;numcnots&#39;]

    Tmax = kwargs[&#39;Tmax&#39;] #Tmax[0] is maximum parameter size, Tmax[1] maximum inftime (timeit),Tmax[2] maximum number of entangling gates
    num_train = len(Y_train)
    validation_size = int(0.1*num_train)
    opt = optim(stepsize=learning_rate) #all optimizers in autograd module take in argument stepsize, so this works for all
    start = time.time()
    for _ in range(kwargs[&#39;nsteps&#39;]):
        batch_index = np.random.randint(0, num_train, (batch_size,))
        X_train_batch = np.asarray(X_train[batch_index])
        Y_train_batch = np.asarray(Y_train[batch_index])
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            var, cost = opt.step_and_cost(lambda v: ohe_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            var, cost = opt.step_and_cost(lambda v: wn_cost_fcn(v, circuit, X_train_batch, Y_train_batch), var)
    end = time.time()
    cost_time = (end - start)

    if kwargs[&#39;rate_type&#39;] == &#39;accuracy&#39;:
        validation_batch = np.random.randint(0, num_train, (validation_size,))
        X_validation_batch = np.asarray(X_train[validation_batch])
        Y_validation_batch = np.asarray(Y_train[validation_batch])
        start = time.time()  # add in timeit function from Wbranch
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            predictions = np.stack([circuit(var, x) for x in X_validation_batch])
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            n = kwargs.get(&#39;nqubits&#39;)
            w = var[:,-1]
            theta = var[:,:-1]
            predictions = [int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,1)) for x in X_validation_batch]
        end = time.time()
        inftime = (end - start) / len(X_validation_batch)
        if kwargs[&#39;readout_layer&#39;]==&#39;one_hot&#39;:
            err_rate = (1.0 - ohe_accuracy(Y_validation_batch,predictions))+10**-7 #add small epsilon to prevent divide by 0 errors
            #print(&#39;error rate:&#39;,err_rate)
            #print(&#39;weights: &#39;,var)
        elif kwargs[&#39;readout_layer&#39;]==&#39;weighted_neuron&#39;:
            err_rate = (1.0 - wn_accuracy(Y_validation_batch,predictions))+10**-7 #add small epsilon to prevent divide by 0 errors
            #print(&#39;error rate:&#39;,err_rate)
            #print(&#39;weights: &#39;,var)
    elif kwargs[&#39;rate_type&#39;] == &#39;batch_cost&#39;:
        err_rate = (cost) + 10**-7 #add small epsilon to prevent divide by 0 errors
        #print(&#39;error rate:&#39;,err_rate)
        #print(&#39;weights: &#39;,var)
        inftime = cost_time
    # QHACK #

    if kwargs[&#39;inf_time&#39;] ==&#39;timeit&#39;:

        W_ = np.abs((Tmax[0] - len(var)) / (Tmax[0])) * np.abs((Tmax[1] - inftime) / (Tmax[1])) * (1. / err_rate)

    elif kwargs[&#39;inf_time&#39;]==&#39;numcnots&#39;:
        nc_ = numcnots
        W_ = np.abs((Tmax[0] - len(var)) / (Tmax[0])) * np.abs((Tmax[2] - nc_) / (Tmax[2])) * (1. / err_rate)

    return W_,var</code></pre>
</details>
</dd>
<dt id="qose.train_utils.wn_accuracy"><code class="name flex">
<span>def <span class="ident">wn_accuracy</span></span>(<span>labels, predictions)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="args">Args</h2>
<dl>
<dt><strong><code>labels</code></strong></dt>
<dd>&nbsp;</dd>
<dt><strong><code>predictions</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wn_accuracy(labels, predictions):
    &#34;&#34;&#34;

    Args:
      labels: 
      predictions: 

    Returns:

    &#34;&#34;&#34;

    loss = 0
    #tol = 0.05
    tol = 0.1
    for l, p in zip(labels, predictions):
        if abs(l - p) &lt; tol:
            loss = loss + 1
    loss = loss / labels.shape[0]

    return loss</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="qose" href="index.html">qose</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="qose.train_utils.evaluate_w" href="#qose.train_utils.evaluate_w">evaluate_w</a></code></li>
<li><code><a title="qose.train_utils.hinge_loss" href="#qose.train_utils.hinge_loss">hinge_loss</a></code></li>
<li><code><a title="qose.train_utils.make_predictions" href="#qose.train_utils.make_predictions">make_predictions</a></code></li>
<li><code><a title="qose.train_utils.mse" href="#qose.train_utils.mse">mse</a></code></li>
<li><code><a title="qose.train_utils.ohe_accuracy" href="#qose.train_utils.ohe_accuracy">ohe_accuracy</a></code></li>
<li><code><a title="qose.train_utils.train_best" href="#qose.train_utils.train_best">train_best</a></code></li>
<li><code><a title="qose.train_utils.train_circuit" href="#qose.train_utils.train_circuit">train_circuit</a></code></li>
<li><code><a title="qose.train_utils.wn_accuracy" href="#qose.train_utils.wn_accuracy">wn_accuracy</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>