{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adjusted-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import pennylane as qml\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import autograd.numpy as np\n",
    "import itertools\n",
    "\n",
    "def train_circuit(circuit,n_params,X_train,Y_train,X_test,Y_test,**kwargs):\n",
    "    \"\"\"Develop and train your very own variational quantum classifier.\n",
    "\n",
    "    Use the provided training data to train your classifier. The code you write\n",
    "    for this challenge should be completely contained within this function\n",
    "    between the # QHACK # comment markers. The number of qubits, choice of\n",
    "    variational ansatz, cost function, and optimization method are all to be\n",
    "    developed by you in this function.\n",
    "\n",
    "    Args:\n",
    "        circuit (qml.QNode): A circuit that you want to train\n",
    "        X_train (np.ndarray): An array of floats of size (M, n) to be used as training data.\n",
    "        Y_train (np.ndarray): An array of size (M,) which are the categorical labels\n",
    "            associated to the training data. The categories are labeled by -1, 0, and 1.\n",
    "        X_test (np.ndarray): An array of floats of (B, n) to serve as testing data.\n",
    "        kwargs: hyperparameters for the training (steps, batch_size, learning_rate)\n",
    "\n",
    "    Returns:\n",
    "        (p,i,e): The number of parameters, the inference time (time to evaluate the accuracy), error rate (accuracy on the test set)\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this array to make a prediction for the labels of the data in X_test\n",
    "    predictions = []\n",
    "\n",
    "    # QHACK #\n",
    "\n",
    "    from autograd.numpy import exp,tanh\n",
    "\n",
    "    # start by renormalizing training data to fit within [-1,1]\n",
    "    \"\"\"\n",
    "    X_train = np.multiply(1.0,np.subtract(np.multiply(\\\n",
    "            np.divide(np.subtract(X_train,X_train.min()),\\\n",
    "                      (X_train.max()-X_train.min())),2.0),1.0))\n",
    "\n",
    "    # start by renormalizing testing data to fit within [-1,1]\n",
    "    X_test = np.multiply(1.0,np.subtract(np.multiply(\\\n",
    "            np.divide(np.subtract(X_test,X_test.min()),\\\n",
    "                      (X_test.max()-X_test.min())),2.0),1.0))\n",
    "    \"\"\"\n",
    "    \n",
    "    def hinge_loss(labels, predictions,type='L2'):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if type=='L1':\n",
    "                loss = loss + np.abs(l - p) # L1 loss\n",
    "            elif type=='L2':\n",
    "                loss = loss + (l - p) ** 2 # L2 loss\n",
    "        loss = loss/len(labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        tol = 0.05\n",
    "        #tol = 0.1\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < tol:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cost_fcn(params,circuit=None,ang_array=[], actual=[]):\n",
    "        '''\n",
    "        use MAE to start\n",
    "        '''\n",
    "        labels = {2:-1,1:1,0:0}\n",
    "        w = params[-3:]\n",
    "        theta = params[:-3]\n",
    "        predictions = [2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, angles=x)))))- 1. for x in ang_array]\n",
    "        return hinge_loss(actual, predictions)\n",
    "\n",
    "    var = np.hstack((np.zeros(n_params),5*np.random.random(3)-2.5))\n",
    "    steps = kwargs['s']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_train = len(Y_train)\n",
    "    validation_size = int(num_train//2)\n",
    "    opt = qml.AdamOptimizer(kwargs['learning_rate'])\n",
    "\n",
    "    for _ in range(steps):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "\n",
    "        var,cost = opt.step_and_cost(lambda v: cost_fcn(v, circuit,X_train_batch, Y_train_batch), var)\n",
    "        w = var[-3:]\n",
    "        theta = var[:-3]\n",
    "\n",
    "    # need timing values from computing predictions\n",
    "    predictions=[int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, angles=x)))))- 1.,0)) for x in X_test]\n",
    "    err_rate = accuracy(predictions,Y_test)\n",
    "    # QHACK #\n",
    "\n",
    "    #return array_to_concatenated_string(predictions)\n",
    "    return len(var),err_rate,'missing time'\n",
    "\n",
    "def classify_data(X_train,Y_train,X_test,Y_test,**kwargs):\n",
    "    \"\"\"Develop and train your very own variational quantum classifier.\n",
    "\n",
    "    Use the provided training data to train your classifier. The code you write\n",
    "    for this challenge should be completely contained within this function\n",
    "    between the # QHACK # comment markers. The number of qubits, choice of\n",
    "    variational ansatz, cost function, and optimization method are all to be\n",
    "    developed by you in this function.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): An array of floats of size (250, 3) to be used as training data.\n",
    "        Y_train (np.ndarray): An array of size (250,) which are the categorical labels\n",
    "            associated to the training data. The categories are labeled by -1, 0, and 1.\n",
    "        X_test (np.ndarray): An array of floats of (50, 3) to serve as testing data.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted categories of X_test, converted from a list of ints to a\n",
    "            comma-separated string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this array to make a prediction for the labels of the data in X_test\n",
    "    predictions = []\n",
    "\n",
    "    # QHACK #\n",
    "\n",
    "    from autograd.numpy import exp,tanh\n",
    "\n",
    "    # start by renormalizing training data to fit within [-1,1]\n",
    "    \"\"\"\n",
    "    X_train = np.multiply(1.0,np.subtract(np.multiply(\\\n",
    "            np.divide(np.subtract(X_train,X_train.min()),\\\n",
    "                      (X_train.max()-X_train.min())),2.0),1.0))\n",
    "\n",
    "    # start by renormalizing testing data to fit within [-1,1]\n",
    "    X_test = np.multiply(1.0,np.subtract(np.multiply(\\\n",
    "            np.divide(np.subtract(X_test,X_test.min()),\\\n",
    "                      (X_test.max()-X_test.min())),2.0),1.0))\n",
    "    \"\"\"\n",
    "    def statepreparation(a):\n",
    "        qml.templates.embeddings.AngleEmbedding(a, wires=range(3), rotation='Y')\n",
    "\n",
    "    def layer(W):\n",
    "        qml.templates.layers.BasicEntanglerLayers(W, wires=range(3), rotation=qml.ops.RY)\n",
    "\n",
    "    def hinge_loss(labels, predictions,type='L2'):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if type=='L1':\n",
    "                loss = loss + np.abs(l - p) # L1 loss\n",
    "            elif type=='L2':\n",
    "                loss = loss + (l - p) ** 2 # L2 loss\n",
    "        loss = loss/len(labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        tol = 0.05\n",
    "        #tol = 0.1\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < tol:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cost_fcn(params,circuit=None,ang_array=[], actual=[]):\n",
    "        '''\n",
    "        use MAE to start\n",
    "        '''\n",
    "        labels = {2:-1,1:1,0:0}\n",
    "        w = params[-3:]\n",
    "        theta = params[:-3]\n",
    "        predictions = [2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, angles=x)))))- 1. for x in ang_array]\n",
    "        return hinge_loss(actual, predictions)\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=3)\n",
    "    @qml.qnode(dev)\n",
    "    def inside_circuit(params,angles=None):\n",
    "        statepreparation(angles)\n",
    "        W= np.reshape(params,(len(params)//3,3))\n",
    "        layer(W)\n",
    "        return qml.expval(qml.PauliZ(0)),qml.expval(qml.PauliZ(1)),qml.expval(qml.PauliZ(2))\n",
    "\n",
    "\n",
    "    var = np.hstack((np.zeros(6),5*np.random.random(3)-2.5))\n",
    "    steps = kwargs['s']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_train = len(Y_train)\n",
    "    validation_size = int(num_train//2)\n",
    "    opt = qml.AdamOptimizer(kwargs['learning_rate'])\n",
    "\n",
    "    for _ in range(steps):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "\n",
    "        var,cost = opt.step_and_cost(lambda v: cost_fcn(v, inside_circuit,X_train_batch, Y_train_batch), var)\n",
    "\n",
    "    # need timing values from computing predictions\n",
    "    theta = var[:-3]\n",
    "    w = var[-3:]\n",
    "    predictions=[int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,inside_circuit(theta, angles=x)))))- 1.,0)) for x in X_test]\n",
    "    err_rate = accuracy(predictions,Y_test)\n",
    "    # QHACK #\n",
    "    return len(var),'time not measured yet',err_rate\n",
    "\n",
    "\n",
    "def array_to_concatenated_string(array):\n",
    "    \"\"\"DO NOT MODIFY THIS FUNCTION.\n",
    "\n",
    "    Turns an array of integers into a concatenated string of integers\n",
    "    separated by commas. (Inverse of concatenated_string_to_array).\n",
    "    \"\"\"\n",
    "    return \",\".join(str(x) for x in array)\n",
    "\n",
    "\n",
    "def concatenated_string_to_array(string):\n",
    "    \"\"\"DO NOT MODIFY THIS FUNCTION.\n",
    "\n",
    "    Turns a concatenated string of integers separated by commas into\n",
    "    an array of integers. (Inverse of array_to_concatenated_string).\n",
    "    \"\"\"\n",
    "    return np.array([int(x) for x in string.split(\",\")])\n",
    "\n",
    "\n",
    "def parse_input(giant_string):\n",
    "    \"\"\"DO NOT MODIFY THIS FUNCTION.\n",
    "\n",
    "    Parse the input data into 3 arrays: the training data, training labels,\n",
    "    and testing data.\n",
    "\n",
    "    Dimensions of the input data are:\n",
    "      - X_train: (250, 3)\n",
    "      - Y_train: (250,)\n",
    "      - X_test:  (50, 3)\n",
    "    \"\"\"\n",
    "    X_train_part, Y_train_part, X_test_part = giant_string.split(\"XXX\")\n",
    "\n",
    "    X_train_row_strings = X_train_part.split(\"S\")\n",
    "    X_train_rows = [[float(x) for x in row.split(\",\")] for row in X_train_row_strings]\n",
    "    X_train = np.array(X_train_rows)\n",
    "\n",
    "    Y_train = concatenated_string_to_array(Y_train_part)\n",
    "\n",
    "    X_test_row_strings = X_test_part.split(\"S\")\n",
    "    X_test_rows = [[float(x) for x in row.split(\",\")] for row in X_test_row_strings]\n",
    "    X_test = np.array(X_test_rows)\n",
    "\n",
    "    return X_train, Y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-thompson",
   "metadata": {},
   "source": [
    "## Import data from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "increased-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "preliminary-scheme",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 1500\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continuing-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = noisy_circles[0][:1000]\n",
    "Y_train = noisy_circles[1][:1000]\n",
    "X_test = noisy_circles[0][500:]\n",
    "Y_test = noisy_circles[1][500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-appliance",
   "metadata": {},
   "source": [
    "### Try running a loop over some hyper_parameters\n",
    "\n",
    "This uses the circuit classifer built during the Challenge -- the circuit and QNode is constructed inside the function `classify_data`\n",
    "\n",
    "The following characteristics are hard wired inside the function `classify_data`:\n",
    "* number of qubits (3)\n",
    "* number of rotation gates (6)\n",
    "* the initialization used for the rotations and weights (rotations intialized with 0, weights initialized with random numbers drawn uniformly from $[-2.5,2.5]$\n",
    "* the rotation gates that are trained (RY)\n",
    "* the rotation gates used in the `AngleEmbeddding` (RY)\n",
    "* the optimizer (`AdamOptimizer`)\n",
    "\n",
    "The following parameters are passed as keywords:\n",
    "* `s` (the number of steps to train for)\n",
    "* `batch_size` (the batch size used in training)\n",
    "* `learning_rate` (the initial learning rate for Adam)\n",
    "\n",
    "As in (de Wynter 2020) we only train each circuit for a few steps (here I'm using 10).  In (de Wynter 2020) the error rate surrogate is defined using a loss function evaluated over a subset of the data -- here I'm using the accuracy of assigned labels over the test data.  The full number of samples that I generated for the dataset is given by `n_samples` (above).  I've split that data in to train,test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "loose-reading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.01), (2, 0.05), (4, 0.01), (4, 0.05), (8, 0.01), (8, 0.05)]\n",
      "2 0.01 (9, 'time not measured yet', 0.442)\n",
      "2 0.05 (9, 'time not measured yet', 0.501)\n",
      "4 0.01 (9, 'time not measured yet', 0.356)\n",
      "4 0.05 (9, 'time not measured yet', 0.501)\n",
      "8 0.01 (9, 'time not measured yet', 0.281)\n",
      "8 0.05 (9, 'time not measured yet', 0.501)\n"
     ]
    }
   ],
   "source": [
    "batch_sets = [2,4,8]\n",
    "learning_rates = [0.01,0.05]\n",
    "hyperparameters = list(itertools.product(batch_sets,learning_rates))\n",
    "print(hyperparameters)\n",
    "for idx,sdx in hyperparameters:\n",
    "    print(idx,sdx,classify_data(X_train,Y_train,X_test,Y_test,s=3,batch_size=idx,learning_rate=sdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "smaller-details",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncommenting and running this cell should cause an error\n",
    "#inside_circuit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "refined-account",
   "metadata": {},
   "source": [
    "### Try running a loop over some hyper_parameters\n",
    "\n",
    "This time use a circuit (QNode) that is created outside the function and passed as an argument\n",
    "\n",
    "Things that are still hard-wired inside the `train_circuit` function:\n",
    "* Optimizer choice\n",
    "* Initialization choice (same as above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "textile-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUBITS = 3 # placeholder\n",
    "def variational_circuit(params,angles=None):\n",
    "    qml.templates.embeddings.AngleEmbedding(angles, wires=range(QUBITS), rotation='Y')\n",
    "    W= np.reshape(params,(len(params)//QUBITS,QUBITS))\n",
    "    qml.templates.layers.BasicEntanglerLayers(W, wires=range(QUBITS), rotation=qml.ops.RY)\n",
    "    return [qml.expval(qml.PauliZ(idx)) for idx in range(QUBITS)]\n",
    "\n",
    "dev = qml.device(\"default.qubit\",wires=QUBITS)\n",
    "# Instantiate the QNode\n",
    "outside_circuit = qml.QNode(func=variational_circuit,device=dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "virtual-fence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<QNode (differentiable): device='default.qubit', func=variational_circuit, wires=3, interface=autograd>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outside_circuit #this should not cause an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "detected-closer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.87758256, 0.77015115, 0.87758256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outside_circuit(np.zeros(6),angles=[0.5,-0.5,0.5]) #this should return something"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "exclusive-methodology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.01), (2, 0.05), (4, 0.01), (4, 0.05), (8, 0.01), (8, 0.05)]\n",
      "2 0.01 (6, 0.499, 'missing time')\n",
      "2 0.05 (6, 0.501, 'missing time')\n",
      "4 0.01 (6, 0.184, 'missing time')\n",
      "4 0.05 (6, 0.711, 'missing time')\n",
      "8 0.01 (6, 0.499, 'missing time')\n",
      "8 0.05 (6, 0.499, 'missing time')\n"
     ]
    }
   ],
   "source": [
    "batch_sets = [2,4,8]\n",
    "learning_rates = [0.01,0.05]\n",
    "hyperparameters = list(itertools.product(batch_sets,learning_rates))\n",
    "print(hyperparameters)\n",
    "for idx,sdx in hyperparameters:\n",
    "    print(idx,sdx,train_circuit(outside_circuit,3,X_train,Y_train,X_test,Y_test,s=3,batch_size=idx,learning_rate=sdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-mayor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
