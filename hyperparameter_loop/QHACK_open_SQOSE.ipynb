{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "musical-algorithm",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fundamental-deviation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "\n",
    "import sys\n",
    "import pennylane as qml\n",
    "import sklearn as skl\n",
    "import autograd.numpy as np\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "def train_circuit_V2(circuit,n_params,n_qubits,X_train,Y_train,rate_type='accuracy',**kwargs):\n",
    "    \"\"\"Develop and train your very own variational quantum classifier.\n",
    "\n",
    "    Use the provided training data to train your classifier. The code you write\n",
    "    for this challenge should be completely contained within this function\n",
    "    between the # QHACK # comment markers. The number of qubits, choice of\n",
    "    variational ansatz, cost function, and optimization method are all to be\n",
    "    developed by you in this function.\n",
    "\n",
    "    Args:\n",
    "        circuit (qml.QNode): A circuit that you want to train\n",
    "        X_train (np.ndarray): An array of floats of size (M, n) to be used as training data.\n",
    "        Y_train (np.ndarray): An array of size (M,) which are the categorical labels\n",
    "            associated to the training data. The categories are labeled by -1, 0, and 1.\n",
    "        X_test (np.ndarray): An array of floats of (B, n) to serve as testing data.\n",
    "        kwargs: hyperparameters for the training (steps, batch_size, learning_rate)\n",
    "\n",
    "    Returns:\n",
    "        (p,i,e,w): The number of parameters, the inference time (time to evaluate the accuracy), error rate (accuracy on the test set)\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this array to make a prediction for the labels of the data in X_test\n",
    "    from autograd.numpy import exp,tanh\n",
    "\n",
    "    def hinge_loss(labels, predictions,type='L2'):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if type=='L1':\n",
    "                loss = loss + np.abs(l - p) # L1 loss\n",
    "            elif type=='L2':\n",
    "                loss = loss + (l - p) ** 2 # L2 loss\n",
    "        loss = loss/len(labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        tol = 0.05\n",
    "        #tol = 0.1\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < tol:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cost_fcn(params,circuit=None,ang_array=[], actual=[]):\n",
    "        '''\n",
    "        use MAE to start\n",
    "        '''\n",
    "        labels = {2:-1,1:1,0:0}\n",
    "        n = len(ang_array[0])\n",
    "        w = params[-n:]\n",
    "        theta = params[:-n]\n",
    "        predictions = [2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1. for x in ang_array]\n",
    "        return hinge_loss(actual, predictions)\n",
    "\n",
    "    var = np.hstack((np.zeros(n_params),5*np.random.random(n_qubits)-2.5))\n",
    "    steps = kwargs['s']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_train = len(Y_train)\n",
    "    validation_size = 5*kwargs['batch_size']\n",
    "    opt = qml.AdamOptimizer(kwargs['learning_rate'])\n",
    "    start = time.time()\n",
    "    for _ in range(steps):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "        var,cost = opt.step_and_cost(lambda v: cost_fcn(v, circuit,X_train_batch, Y_train_batch), var)\n",
    "    end = time.time()\n",
    "    cost_time = (end-start)\n",
    "\n",
    "    w = var[-n_qubits:]\n",
    "    theta = var[:-n_qubits]\n",
    "\n",
    "    if rate_type =='accuracy':\n",
    "        validation_batch = np.random.randint(0, num_train, (validation_size,))\n",
    "        X_validation_batch = X_train[validation_batch]\n",
    "        Y_validation_batch = Y_train[validation_batch]\n",
    "        start = time.time() # add in timeit function from Wbranch\n",
    "        predictions=[int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,0)) for x in X_validation_batch]\n",
    "        end = time.time()\n",
    "        inftime = (end-start)/len(X_validation_batch)\n",
    "        err_rate = 1.0 - accuracy(predictions,Y_validation_batch)\n",
    "    elif rate_type=='batch_cost':\n",
    "        err_rate = cost\n",
    "        inftime = cost_time\n",
    "    # QHACK #\n",
    "    W_ = np.abs((100.-len(var))/(100.))*np.abs((100.-inftime)/(100.))*(1./err_rate)\n",
    "    return len(var),inftime,err_rate,W_,var\n",
    "\n",
    "def train_circuit(circuit,n_params,n_qubits,X_train,Y_train,rate_type='accuracy',**kwargs):\n",
    "    \"\"\"Develop and train your very own variational quantum classifier.\n",
    "\n",
    "    Use the provided training data to train your classifier. The code you write\n",
    "    for this challenge should be completely contained within this function\n",
    "    between the # QHACK # comment markers. The number of qubits, choice of\n",
    "    variational ansatz, cost function, and optimization method are all to be\n",
    "    developed by you in this function.\n",
    "\n",
    "    Args:\n",
    "        circuit (qml.QNode): A circuit that you want to train\n",
    "        X_train (np.ndarray): An array of floats of size (M, n) to be used as training data.\n",
    "        Y_train (np.ndarray): An array of size (M,) which are the categorical labels\n",
    "            associated to the training data. The categories are labeled by -1, 0, and 1.\n",
    "        X_test (np.ndarray): An array of floats of (B, n) to serve as testing data.\n",
    "        kwargs: hyperparameters for the training (steps, batch_size, learning_rate)\n",
    "\n",
    "    Returns:\n",
    "        (p,i,e,w): The number of parameters, the inference time (time to evaluate the accuracy), error rate (accuracy on the test set)\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this array to make a prediction for the labels of the data in X_test\n",
    "    from autograd.numpy import exp,tanh\n",
    "\n",
    "    def hinge_loss(labels, predictions,type='L2'):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if type=='L1':\n",
    "                loss = loss + np.abs(l - p) # L1 loss\n",
    "            elif type=='L2':\n",
    "                loss = loss + (l - p) ** 2 # L2 loss\n",
    "        loss = loss/len(labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        tol = 0.05\n",
    "        #tol = 0.1\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < tol:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cost_fcn(params,circuit=None,ang_array=[], actual=[],r=1):\n",
    "        '''\n",
    "        use MAE to start\n",
    "        '''\n",
    "        labels = {2:-1,1:1,0:0}\n",
    "        n = len(ang_array[0])\n",
    "        w = params[-n*r:]\n",
    "        theta = params[:-n*r]\n",
    "        predictions = [2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1. for x in ang_array]\n",
    "        return hinge_loss(actual, predictions)\n",
    "\n",
    "    var = np.hstack((np.zeros(n_params),5*np.random.random(n_qubits)-2.5))\n",
    "    steps = kwargs['s']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_train = len(Y_train)\n",
    "    validation_size = 5*kwargs['batch_size']\n",
    "    opt = qml.AdamOptimizer(kwargs['learning_rate'])\n",
    "    start = time.time()\n",
    "    r=n_qubits//X_train.shape[1]\n",
    "    for _ in range(steps):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "        var,cost = opt.step_and_cost(lambda v: cost_fcn(v, circuit,X_train_batch, Y_train_batch,r), var)\n",
    "        print(_,cost)\n",
    "    end = time.time()\n",
    "    cost_time = (end-start)\n",
    "\n",
    "    w = var[-n_qubits:]\n",
    "    theta = var[:-n_qubits]\n",
    "\n",
    "    if rate_type =='accuracy':\n",
    "        validation_batch = np.random.randint(0, num_train, (validation_size,))\n",
    "        X_validation_batch = X_train[validation_batch]\n",
    "        Y_validation_batch = Y_train[validation_batch]\n",
    "        start = time.time() # add in timeit function from Wbranch\n",
    "        predictions=[int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, features=x)))))- 1.,0)) for x in X_validation_batch]\n",
    "        end = time.time()\n",
    "        inftime = (end-start)/len(X_validation_batch)\n",
    "        err_rate = 1.0 - accuracy(predictions,Y_validation_batch) + 10**-6 # add epsilon to avoid division by 0\n",
    "    elif rate_type=='batch_cost':\n",
    "        err_rate = cost\n",
    "        inftime = cost_time\n",
    "    # QHACK #\n",
    "    W_ = np.abs((100.-len(var))/(100.))*np.abs((100.-inftime)/(100.))*(1./err_rate)\n",
    "    #return len(var),inftime,err_rate,W_,var\n",
    "    return W_,var\n",
    "\n",
    "def loop_over_hyperparameters(circuit,n_params,X_train,Y_train,batch_sets,learning_rates,**kwargs):\n",
    "    \"\"\"\n",
    "    together with the function train_circuit(...) this executes lines 7-8 in the Algorithm 1 pseudo code of (de Wynter 2020)\n",
    "    \"\"\"\n",
    "    hyperparameter_space = list(itertools.product(batch_sets,learning_rates))\n",
    "    Wmax = 0.0\n",
    "    s = kwargs.get('s', None)\n",
    "    rate_type = kwargs.get('rate_type',None)\n",
    "    \n",
    "    for idx,sdx in hyperparameters:\n",
    "        wtemp,weights=train_circuit(circuit,n_params,X_train,Y_train,s=s,batch_size=idx,rate_type=rate_type,learning_rate=sdx)\n",
    "        if wtemp>=Wmax:\n",
    "            Wmax=wtemp\n",
    "            saved_weights = weights\n",
    "    return Wmax,saved_weights\n",
    "\n",
    "def classify_data_DEPRECIATED(X_train,Y_train,X_test,Y_test,**kwargs):\n",
    "    \"\"\"Develop and train your very own variational quantum classifier.\n",
    "\n",
    "    Use the provided training data to train your classifier. The code you write\n",
    "    for this challenge should be completely contained within this function\n",
    "    between the # QHACK # comment markers. The number of qubits, choice of\n",
    "    variational ansatz, cost function, and optimization method are all to be\n",
    "    developed by you in this function.\n",
    "\n",
    "    Args:\n",
    "        X_train (np.ndarray): An array of floats of size (250, 3) to be used as training data.\n",
    "        Y_train (np.ndarray): An array of size (250,) which are the categorical labels\n",
    "            associated to the training data. The categories are labeled by -1, 0, and 1.\n",
    "        X_test (np.ndarray): An array of floats of (50, 3) to serve as testing data.\n",
    "\n",
    "    Returns:\n",
    "        str: The predicted categories of X_test, converted from a list of ints to a\n",
    "            comma-separated string.\n",
    "    \"\"\"\n",
    "\n",
    "    # Use this array to make a prediction for the labels of the data in X_test\n",
    "    predictions = []\n",
    "\n",
    "    # QHACK #\n",
    "\n",
    "    from autograd.numpy import exp,tanh\n",
    "\n",
    "    def statepreparation(a):\n",
    "        qml.templates.embeddings.AngleEmbedding(a, wires=range(3), rotation='Y')\n",
    "\n",
    "    def layer(W):\n",
    "        qml.templates.layers.BasicEntanglerLayers(W, wires=range(3), rotation=qml.ops.RY)\n",
    "\n",
    "    def hinge_loss(labels, predictions,type='L2'):\n",
    "        loss = 0\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if type=='L1':\n",
    "                loss = loss + np.abs(l - p) # L1 loss\n",
    "            elif type=='L2':\n",
    "                loss = loss + (l - p) ** 2 # L2 loss\n",
    "        loss = loss/len(labels)\n",
    "        return loss\n",
    "\n",
    "    def accuracy(labels, predictions):\n",
    "\n",
    "        loss = 0\n",
    "        tol = 0.05\n",
    "        #tol = 0.1\n",
    "        for l, p in zip(labels, predictions):\n",
    "            if abs(l - p) < tol:\n",
    "                loss = loss + 1\n",
    "        loss = loss / len(labels)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def cost_fcn(params,circuit=None,ang_array=[], actual=[]):\n",
    "        '''\n",
    "        use MAE to start\n",
    "        '''\n",
    "        labels = {2:-1,1:1,0:0}\n",
    "        w = params[-3:]\n",
    "        theta = params[:-3]\n",
    "        predictions = [2.*(1.0/(1.0+exp(np.dot(-w,circuit(theta, angles=x)))))- 1. for x in ang_array]\n",
    "        return hinge_loss(actual, predictions)\n",
    "\n",
    "    dev = qml.device(\"default.qubit\", wires=3)\n",
    "    @qml.qnode(dev)\n",
    "    def inside_circuit(params,angles=None):\n",
    "        statepreparation(angles)\n",
    "        W= np.reshape(params,(len(params)//3,3))\n",
    "        layer(W)\n",
    "        return qml.expval(qml.PauliZ(0)),qml.expval(qml.PauliZ(1)),qml.expval(qml.PauliZ(2))\n",
    "\n",
    "\n",
    "    var = np.hstack((np.zeros(6),5*np.random.random(3)-2.5))\n",
    "    steps = kwargs['s']\n",
    "    batch_size = kwargs['batch_size']\n",
    "    num_train = len(Y_train)\n",
    "    validation_size = int(num_train//2)\n",
    "    opt = qml.AdamOptimizer(kwargs['learning_rate'])\n",
    "\n",
    "    for _ in range(steps):\n",
    "        batch_index = np.random.randint(0, num_train, (batch_size,))\n",
    "        X_train_batch = X_train[batch_index]\n",
    "        Y_train_batch = Y_train[batch_index]\n",
    "        var,cost = opt.step_and_cost(lambda v: cost_fcn(v, inside_circuit,X_train_batch, Y_train_batch,r), var)\n",
    "\n",
    "    # need timing values from computing predictions\n",
    "\n",
    "    \n",
    "    theta = var[:-3]\n",
    "    w = var[-3:]\n",
    "    start = time.time() # add in timeit function from Wbranch\n",
    "    predictions=[int(np.round(2.*(1.0/(1.0+exp(np.dot(-w,inside_circuit(theta, angles=x)))))- 1.,0)) for x in X_test]\n",
    "    end = time.time()\n",
    "    inftime = end-start\n",
    "    err_rate = 1.0 - accuracy(predictions,Y_test)\n",
    "    # QHACK #\n",
    "    W_ = len(var)*inftime*(1./err_rate)\n",
    "    return len(var),inftime,err_rate,W_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-tampa",
   "metadata": {},
   "source": [
    "## Import data from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "informal-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "desirable-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "resident-virginia",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = noisy_circles[0]\n",
    "Y_train = noisy_circles[1]\n",
    "X_test = noisy_circles[0][50:]\n",
    "Y_test = noisy_circles[1][50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lightweight-spanking",
   "metadata": {},
   "source": [
    "### Try running a loop over some hyper_parameters\n",
    "\n",
    "This uses the circuit classifer built during the Challenge -- the circuit and QNode is constructed inside the function `classify_data`\n",
    "\n",
    "The following characteristics are hard wired inside the function `classify_data`:\n",
    "* number of qubits (3)\n",
    "* number of rotation gates (6)\n",
    "* the initialization used for the rotations and weights (rotations intialized with 0, weights initialized with random numbers drawn uniformly from $[-2.5,2.5]$\n",
    "* the rotation gates that are trained (RY)\n",
    "* the rotation gates used in the `AngleEmbeddding` (RY)\n",
    "* the optimizer (`AdamOptimizer`)\n",
    "\n",
    "The following parameters are passed as keywords:\n",
    "* `s` (the number of steps to train for)\n",
    "* `batch_size` (the batch size used in training)\n",
    "* `learning_rate` (the initial learning rate for Adam)\n",
    "\n",
    "As in (de Wynter 2020) we only train each circuit for a few steps (here I'm using 10).  In (de Wynter 2020) the error rate surrogate is defined using a loss function evaluated over a subset of the data -- here I'm using the accuracy of assigned labels over the test data.  The full number of samples that I generated for the dataset is given by `n_samples` (above).  I've split that data in to train,test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-bundle",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sets = [2,4,8]\n",
    "learning_rates = [0.01,0.05]\n",
    "hyperparameters = list(itertools.product(batch_sets,learning_rates))\n",
    "print(hyperparameters)\n",
    "for idx,sdx in hyperparameters:\n",
    "    print(idx,sdx,classify_data(X_train,Y_train,X_test,Y_test,s=3,batch_size=idx,learning_rate=sdx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-supplier",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uncommenting and running this cell should cause an error\n",
    "#inside_circuit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saving-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dirty-stand",
   "metadata": {},
   "source": [
    "### Try running a loop over some hyper_parameters\n",
    "\n",
    "This time use a circuit (QNode) that is created outside the function and passed as an argument\n",
    "\n",
    "Things that are still hard-wired inside the `train_circuit` function:\n",
    "* Optimizer choice\n",
    "* Initialization choice (same as above)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variational_circuit(params,features=None):\n",
    "    r_ = QUBITS//len(features)\n",
    "    large_features = np.tile(features,r_)\n",
    "    qml.templates.embeddings.AngleEmbedding(large_features, wires=range(QUBITS), rotation='Y') # replace with more general embedding\n",
    "    if len(params)%QUBITS!=0:\n",
    "        print('chooose parameter length that is divisible by number of qubits')\n",
    "        return\n",
    "    W= np.reshape(params,(len(params)//QUBITS,QUBITS))\n",
    "    qml.templates.layers.BasicEntanglerLayers(W, wires=range(QUBITS), rotation=qml.ops.RY)\n",
    "    return [qml.expval(qml.PauliZ(idx)) for idx in range(QUBITS)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-subject",
   "metadata": {},
   "outputs": [],
   "source": [
    "qubit_sizes=[2,4]\n",
    "batch_sets = [2,4,8,16,32]\n",
    "learning_rates = [0.001,0.005,0.01,0.05]\n",
    "hyperparameters = list(itertools.product(batch_sets,learning_rates))\n",
    "print(hyperparameters)\n",
    "results = {}\n",
    "Wmax = 0.0\n",
    "for qdx in qubit_sizes:\n",
    "    QUBITS=qdx\n",
    "    dev = qml.device(\"default.qubit\",wires=QUBITS)\n",
    "    # Instantiate the QNode\n",
    "    outside_circuit = qml.QNode(func=variational_circuit,device=dev)\n",
    "    n_params=2*QUBITS\n",
    "    for idx,sdx in hyperparameters:\n",
    "        p,i,er,wtemp,weights=train_circuit(outside_circuit,n_params,qdx,X_train,Y_train,s=5,batch_size=idx,rate_type='accuracy',learning_rate=sdx)\n",
    "        print(p,i,er,wtemp,weights)\n",
    "        if wtemp>=Wmax:\n",
    "            Wmax=wtemp\n",
    "            saved_weights = weights\n",
    "            output = (idx,sdx,p,i,er)\n",
    "print(\"Max W coef: \", Wmax)\n",
    "print(\"saved weights: \",saved_weights)\n",
    "print(\"hyperparameters: \",output[0],output[1])\n",
    "print(\"variables (p,i,er): \",p,i,er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "loop_over_hyperparameters(outside_circuit,4,X_train,Y_train,batch_sets=[2,4,8,16,32],learning_rates=[0.001,0.005,0.01,0.05],s=5,rate_type='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-fourth",
   "metadata": {},
   "source": [
    "### Pull in more detailed circuit design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-quantum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pennylane as qml\n",
    "import sklearn as skl\n",
    "import autograd.numpy as np\n",
    "import itertools\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "increasing-genius",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def zz_layer(wires, params):\n",
    "    nq = len(wires)\n",
    "    for n in range(nq - 1):\n",
    "        zz_gate([n, n + 1], params[n])\n",
    "    zz_gate([nq - 1, 0], params[nq - 1])\n",
    "\n",
    "\n",
    "def zz_gate(wires, gamma):\n",
    "    if isinstance(gamma,(float,qml.variable.Variable)):\n",
    "        qml.CNOT(wires=wires)\n",
    "        qml.RZ(gamma, wires=wires[1])\n",
    "        qml.CNOT(wires=wires)\n",
    "    else:\n",
    "        qml.CNOT(wires=wires)\n",
    "        qml.RZ(gamma._value, wires=wires[1])\n",
    "        qml.CNOT(wires=wires)\n",
    "\n",
    "def CNOT_layer(wires):\n",
    "    nq = len(wires)\n",
    "    for n in range(nq-1):\n",
    "        qml.CNOT(wires=[n,n+1])\n",
    "    qml.CNOT(wires=[nq-1,0])\n",
    "\n",
    "def x_layer(wires, params):\n",
    "    nqubits = len(wires)\n",
    "    if isinstance(params,(list,np.ndarray,qml.variable.Variable)):\n",
    "        for n in range(nqubits):\n",
    "            qml.RX(params[n], wires=[n, ])\n",
    "    else:\n",
    "        for n in range(nqubits):\n",
    "            qml.RX(params[n]._value, wires=[n, ])\n",
    "\n",
    "\n",
    "def y_layer(wires, params):\n",
    "    nqubits = len(wires)\n",
    "    if isinstance(params,(list,np.ndarray,qml.variable.Variable)):\n",
    "        for n in range(nqubits):\n",
    "            qml.RY(params[n], wires=[n, ])\n",
    "    else:\n",
    "        for n in range(nqubits):\n",
    "            qml.RY(params[n]._value, wires=[n, ])\n",
    "\n",
    "#TODO: ADD W-COST HERE\n",
    "\n",
    "string_to_layer_mapping = {'ZZ': zz_layer, 'X': x_layer, 'Y': y_layer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "vulnerable-novelty",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def string2circuit(gate_params=[],features=[],arch_params=[],n_wires=None):\n",
    "    string_to_param_count = {'E1':0,'ZZ':n_wires,'X':n_wires,'Y':n_wires}\n",
    "    pdx = 0\n",
    "    for gdx in arch_params:\n",
    "        if gdx=='E1':\n",
    "            r_ = n_wires//len(features)\n",
    "            large_features = np.tile(features,r_)\n",
    "            qml.templates.embeddings.AngleEmbedding(large_features, wires=range(n_wires), rotation='Y') # replace with more general embedding\n",
    "            #qml.templates.embeddings.AngleEmbedding(features, wires=range(n_wires), rotation='Y') # replace with more general embedding\n",
    "        elif gdx=='ZZ':\n",
    "            #w = gate_params[pdx:pdx+n_wires]\n",
    "            #zz_layer(range(n_wires),w)\n",
    "            CNOT_layer(range(n_wires))\n",
    "            #pdx+=n_wires\n",
    "        elif gdx=='X':\n",
    "            w = gate_params[pdx:pdx+n_wires]\n",
    "            x_layer(range(n_wires),w)\n",
    "            pdx+=n_wires\n",
    "        elif gdx=='Y':\n",
    "            w = gate_params[pdx:pdx+n_wires]\n",
    "            y_layer(range(n_wires),w)\n",
    "            pdx+=n_wires\n",
    "    return [qml.expval(qml.PauliZ(idx)) for idx in range(n_wires)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "judicial-valentine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, 0.001), (2, 0.005), (2, 0.01), (2, 0.05), (4, 0.001), (4, 0.005), (4, 0.01), (4, 0.05), (8, 0.001), (8, 0.005), (8, 0.01), (8, 0.05), (16, 0.001), (16, 0.005), (16, 0.01), (16, 0.05), (32, 0.001), (32, 0.005), (32, 0.01), (32, 0.05)]\n",
      "10\n",
      "0 0.13752273400732473\n",
      "1 0.1927262072942658\n",
      "2 0.2878470875055854\n",
      "3 0.21109554010146525\n",
      "4 0.10298211283225189\n",
      "final weights: [ 0.00169041 -0.00270752 -0.00100336 -0.00295001 -0.00267867 -0.00208673\n",
      "  0.00251852 -0.00270768 -0.00100259 -0.00295052  0.64650615  0.46085116]\n",
      "8.48897500158883\n",
      "0 0.3567203317478812\n",
      "1 0.018604066645504083\n",
      "2 0.35217352240216976\n",
      "3 0.3422041323253365\n",
      "4 0.6738125291825741\n",
      "final weights: [ 0.00638273  0.01410457  0.01519151 -0.01529088  0.01409094  0.01506289\n",
      "  0.00578289  0.01410562  0.01519714 -0.01533379  0.15833052  0.25851329]\n",
      "1.297454808929577\n",
      "0 0.19272582415664935\n",
      "1 0.17345870548721962\n",
      "2 0.2625015411978689\n",
      "3 0.2878416762870544\n",
      "4 0.131249061586121\n",
      "final weights: [-1.34772010e-03 -3.18868454e-02 -3.06771405e-02  3.12186920e-02\n",
      " -3.15483138e-02 -3.10369869e-02  2.06447584e-02 -3.19553993e-02\n",
      " -3.06754626e-02  3.05080585e-02  2.00560992e+00  7.32782619e-02]\n",
      "6.661874387271572\n",
      "0 1.8096955836175828\n",
      "1 0.9141427249953491\n",
      "2 1.5403277436582734\n",
      "3 0.7058574578867869\n",
      "4 1.3461390916539413\n",
      "final weights: [ 0.14667481 -0.16045604 -0.21295386  0.20248465 -0.16134627 -0.21617653\n",
      "  0.14982623 -0.16009432 -0.21957305  0.19507666 -0.06037802 -0.41068616]\n",
      "0.6493794262667563\n",
      "0 0.5112378327241949\n",
      "1 0.24639955618498\n",
      "2 0.5039342618292402\n",
      "3 0.9977158685131332\n",
      "4 0.49654713620997337\n",
      "final weights: [ 0.00303927  0.00264146  0.00042669 -0.00357409  0.00289679  0.00040739\n",
      "  0.00289122  0.00264309  0.00042659 -0.00357492 -0.33659166  0.31782749]\n",
      "1.7493564676489812\n",
      "0 0.22087282617238396\n",
      "1 0.14931806906095405\n",
      "2 0.337873875166608\n",
      "3 0.39289629632664114\n",
      "4 0.21572313745292848\n",
      "final weights: [ 0.01317066 -0.01669194 -0.01587703  0.01308011 -0.01771583 -0.01601978\n",
      "  0.01271569 -0.01669872 -0.01588656  0.01311152 -0.52206379  2.22049249]\n",
      "4.024109239384886\n",
      "0 1.2128889420157871\n",
      "1 1.143729127778653\n",
      "2 2.6587268010258738\n",
      "3 1.8927828871584695\n",
      "4 2.619074627804508\n",
      "final weights: [-0.02987478 -0.03226018  0.00583512 -0.0331416  -0.03253906 -0.03411096\n",
      " -0.03320855 -0.03225629  0.00603052 -0.03254914 -2.21566973 -0.50470589]\n",
      "0.33154755360909566\n",
      "0 0.014549900389744854\n",
      "1 0.7816317710945764\n",
      "2 0.5824816030653981\n",
      "3 0.5131291637986036\n",
      "4 0.6576391293947768\n",
      "final weights: [-0.10970677  0.06741799  0.17748631  0.11405234 -0.20383342  0.18008137\n",
      " -0.11486659  0.11224273  0.17532991  0.10936446 -1.13223684  1.17742641]\n",
      "1.3204193982498047\n",
      "0 0.27569734049948963\n",
      "1 0.40226621330114054\n",
      "2 0.4361348341006428\n",
      "3 0.19697249135776385\n",
      "4 0.4630504238795773\n",
      "final weights: [ 0.00322946 -0.00255588 -0.00305783 -0.00335272 -0.00302419 -0.00302524\n",
      "  0.00324205 -0.00255696 -0.00305765 -0.00335312 -0.75478695  1.2076588 ]\n",
      "1.8467442843948418\n",
      "0 0.1676818488983107\n",
      "1 0.17363302984957374\n",
      "2 0.24963641660994695\n",
      "3 0.20497566067064416\n",
      "4 0.20577449812805615\n",
      "final weights: [-0.01669681  0.01249312  0.01528624 -0.00417255  0.01250512  0.01508008\n",
      " -0.01655181  0.01248476  0.01534757 -0.0043325   0.42597133  1.35370386]\n",
      "4.1602214781263065\n",
      "0 0.38050360389028887\n",
      "1 0.35335396675885533\n",
      "2 0.42596896223714426\n",
      "3 0.44961417354998146\n",
      "4 0.5891769927876738\n",
      "final weights: [-0.03343372  0.03144206 -0.0158769  -0.03136093  0.03139053  0.03274772\n",
      " -0.0329158   0.03144787 -0.01571001 -0.03129878  1.79136882  2.08716616]\n",
      "1.4548347208540373\n",
      "0 1.6870671955008651\n",
      "1 1.9952913649031498\n",
      "2 2.6370957464781384\n",
      "3 1.3288374031602603\n",
      "4 1.1197584348227834\n",
      "final weights: [ 0.16086355 -0.18886762  0.16602164  0.22578505 -0.17708766  0.18411308\n",
      "  0.16142674 -0.19052707  0.17194637  0.22330649 -0.69453443 -1.74947638]\n",
      "0.7651041411127739\n",
      "0 1.3665422968879188\n",
      "1 2.6841187113844858\n",
      "2 2.276950598046383\n",
      "3 1.7547423160192137\n",
      "4 1.5607547130405317\n",
      "final weights: [-0.00323771  0.00350765 -0.00333235 -0.00371589  0.00355384 -0.00329758\n",
      " -0.00326353  0.0035083  -0.00333209 -0.00370873 -1.72847468 -0.99441371]\n",
      "0.5329295757489\n",
      "0 0.1440059518988762\n",
      "1 0.17599814604934508\n",
      "2 0.3476034418878491\n",
      "3 0.15746592818996302\n",
      "4 0.21283389509472575\n",
      "final weights: [ 0.01287771  0.01406341 -0.01528249  0.01596373  0.01364923 -0.0149828\n",
      "  0.01318746  0.01406422 -0.0152836   0.01724312  2.39964365  0.16480033]\n",
      "3.906590070067379\n",
      "0 0.7113235405169452\n",
      "1 1.0308814748167356\n",
      "2 0.571193295190326\n",
      "3 1.1131336401972338\n",
      "4 1.0995799147091068\n",
      "final weights: [-0.02336873  0.02965477  0.0282656   0.02654497  0.02973001  0.02815698\n",
      " -0.02612208  0.02966441  0.02819547  0.02676866 -1.5298842   0.71787626]\n",
      "0.7581533603670253\n",
      "0 0.24875618078749714\n",
      "1 0.23597489717254777\n",
      "2 0.2544247999142888\n",
      "3 0.3080531717840587\n",
      "4 0.24371964538857255\n",
      "final weights: [ 0.04821733 -0.12939573 -0.15196722  0.17734309 -0.13499741 -0.15247773\n",
      "  0.04798199 -0.12699379 -0.15168478  0.16205366 -0.95692901  2.32381489]\n",
      "3.4068277944734184\n",
      "0 0.1411412748857915\n",
      "1 0.14893248406528073\n",
      "2 0.14727995765083332\n",
      "3 0.15213243429309561\n",
      "4 0.1423646286593085\n",
      "final weights: [ 0.00307375 -0.00351362 -0.00324827 -0.00305295 -0.00351635 -0.00329837\n",
      "  0.00337511 -0.00351307 -0.00324846 -0.00305192  2.47041096 -0.72474705]\n",
      "5.501209451389766\n",
      "0 0.35298512143879235\n",
      "1 0.4016771417819707\n",
      "2 0.48631116529086194\n",
      "3 0.4047942050542433\n",
      "4 0.2826194836338665\n",
      "final weights: [0.016156   0.01853277 0.00781    0.01267765 0.0184953  0.00581632\n",
      " 0.01621494 0.01852466 0.00783319 0.01309784 2.04947145 1.89073001]\n",
      "2.788556889904288\n",
      "0 0.6727911371374865\n",
      "1 0.6183531168556804\n",
      "2 0.7204996502721496\n",
      "3 0.5854649245998911\n",
      "4 0.5308551711816298\n",
      "final weights: [ 0.03314068  0.03005521  0.03538646 -0.02493144  0.03029769  0.03556061\n",
      "  0.03293567  0.02998424  0.03538569 -0.02530112 -0.77033181  0.58384116]\n",
      "1.4835119439465445\n",
      "0 2.2962547338342714\n",
      "1 2.0724182335969026\n",
      "2 1.89752596722385\n",
      "3 1.6054633376247518\n",
      "4 1.6437134013010255\n",
      "final weights: [-0.16018488  0.17913591  0.20666254 -0.22797462  0.1773797   0.21584767\n",
      " -0.16024945  0.17976096  0.20863909 -0.22976945 -1.06498128 -1.82369179]\n",
      "0.47713065956785683\n",
      "Max W coef:  8.48897500158883\n",
      "saved weights:  [ 0.00169041 -0.00270752 -0.00100336 -0.00295001 -0.00267867 -0.00208673\n",
      "  0.00251852 -0.00270768 -0.00100259 -0.00295052  0.64650615  0.46085116]\n",
      "20\n",
      "0 0.4281772117202304\n",
      "1 0.5317781666011903\n",
      "2 0.036594253531534934\n",
      "3 0.06495747504763419\n",
      "4 0.05549554537626553\n",
      "final weights: [-4.82830323e-04 -1.54640138e-03  4.60331607e-04  3.43187733e-03\n",
      "  2.08566884e-03 -3.34945539e-03  8.69318183e-04  3.01937090e-03\n",
      "  3.06163798e-03  3.15860240e-03 -5.43815407e-04 -3.20614717e-03\n",
      " -7.90299463e-04  3.29276574e-03  6.38029944e-05 -6.57195855e-04\n",
      " -4.76927360e-04  4.59006397e-04  3.44085987e-03 -2.76262414e-03\n",
      "  9.93067900e-02 -2.21278898e+00  1.30399576e+00  1.62934887e+00]\n",
      "13.322890786464272\n",
      "0 0.1352878613994324\n",
      "1 0.15440737151999379\n",
      "2 0.12375897938022486\n",
      "3 0.20944166951529378\n",
      "4 0.14819630509694676\n",
      "final weights: [ 1.30892311e-02 -1.31402751e-02  1.36882086e-02  1.42838436e-02\n",
      "  3.58150184e-03  1.50986039e-02 -2.95721190e-03 -4.69749524e-04\n",
      " -3.06403658e-03  1.11943456e-03  1.31746544e-02  1.37883477e-02\n",
      "  1.75555330e-02  1.37183906e-02  1.78583295e-02  1.34584180e-02\n",
      "  1.35671264e-02  1.34305541e-02  1.43437712e-02  1.95266235e-02\n",
      "  2.67564621e-01  1.21760415e-01  1.00778915e+00  1.82158382e-01]\n",
      "4.989163565724378\n",
      "0 0.4017934104055637\n",
      "1 2.0223919301328186\n",
      "2 0.4065019945239128\n",
      "3 2.056899535196438\n",
      "4 2.0228333061244035\n",
      "final weights: [ 0.03120681  0.01654583  0.03029299  0.02919859  0.02989697  0.03244513\n",
      "  0.0311258   0.00205843  0.03335407 -0.02544227  0.03144196  0.02503046\n",
      "  0.02994683  0.02458772  0.03015167  0.03066898  0.02993503  0.03004473\n",
      "  0.02905148  0.03505057 -0.88958759 -1.3434279  -1.40895361 -0.22141689]\n",
      "0.3658226357677837\n",
      "0 0.9556853956618611\n",
      "1 0.85674509927342\n",
      "2 0.036229175686418494\n",
      "3 0.6329420569117788\n",
      "4 1.3977019421790189\n",
      "final weights: [ 0.15647702  0.10485692  0.16858841  0.11109371  0.15124295 -0.16008474\n",
      " -0.10081608  0.14094527  0.14326783 -0.0488242   0.1738702   0.15309915\n",
      " -0.15526235 -0.16139083  0.11711088 -0.14973852 -0.14933754  0.1662598\n",
      "  0.10960802  0.20448357  1.13859181 -0.8007073   0.47372803 -1.10412364]\n",
      "0.5294160210609254\n",
      "0 0.7055720035580111\n",
      "1 1.9799305719603484\n",
      "2 1.3570597852829729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 1.9045439128166224\n",
      "4 1.3748105400963446\n",
      "final weights: [ 3.51255048e-03  1.42197946e-03  3.50238609e-03 -3.20196042e-03\n",
      "  3.27564331e-03  3.38104273e-03 -1.43448675e-03 -2.97366915e-03\n",
      " -2.62548217e-03 -1.90015396e-03  3.48734173e-03 -2.62779997e-03\n",
      "  2.91290826e-03  2.14301248e-03  2.94924224e-03  4.86605401e-03\n",
      "  3.45368182e-03  3.50494778e-03 -3.20577041e-03  4.64542825e-03\n",
      " -2.10371399e-02 -7.17287881e-01  1.21775864e+00 -2.14736171e+00]\n",
      "0.5228686504094435\n",
      "0 0.3195892531535749\n",
      "1 0.40380270173221755\n",
      "2 0.32212487574462334\n",
      "3 0.2949472824965064\n",
      "4 0.4845788928410664\n",
      "final weights: [ 0.01578626  0.01636465 -0.01449126 -0.02008728  0.01628477 -0.01662682\n",
      " -0.00384255  0.01329518  0.01012396 -0.00663425  0.01619006 -0.01287872\n",
      "  0.01526652 -0.01347198  0.01563209 -0.01608487 -0.01532038 -0.01454695\n",
      " -0.02009254  0.01836382  2.24427152  0.93218082 -0.90401962 -1.74934669]\n",
      "1.4833359408837181\n",
      "0 0.27536259855030265\n",
      "1 0.19670528316641275\n",
      "2 0.1656983000788766\n",
      "3 0.4507829476243292\n",
      "4 0.14661574548010062\n",
      "final weights: [ 2.33448316e-02 -7.29378508e-03  1.82851291e-02  9.54818841e-03\n",
      " -9.20566501e-03 -1.54571698e-02 -3.21633543e-02 -3.10201839e-02\n",
      " -9.62319896e-03 -1.70194110e-02 -2.03444634e-03 -3.91762694e-02\n",
      "  2.81218601e-02 -3.82937296e-02  2.96048279e-02 -2.00858683e-04\n",
      "  1.96619820e-02  1.91315731e-02  9.81619667e-03  3.39636581e-02\n",
      "  1.68285593e+00  9.11118977e-01 -2.16878868e+00  2.60003400e-01]\n",
      "4.903540216529139\n",
      "0 0.3671329694484946\n",
      "1 0.42258169908201193\n",
      "2 0.0691309368655443\n",
      "3 0.19082238651660868\n",
      "4 0.19784212159792117\n",
      "final weights: [ 0.01169384 -0.08268771 -0.18310152 -0.16863847 -0.11709289 -0.17489428\n",
      "  0.10962634 -0.0418723   0.06939309  0.05121294 -0.12224083 -0.16339611\n",
      "  0.1547448   0.14841592 -0.15501747 -0.02039868  0.04307553 -0.18553683\n",
      " -0.17921109  0.15089873  0.99951904 -2.04254599 -0.42868199  2.56106122]\n",
      "3.6434336625996107\n",
      "0 2.8535667856437783\n",
      "1 2.8611909742036987\n",
      "2 0.8704861343767649\n",
      "3 0.8193907488504951\n",
      "4 1.2839023387705266\n",
      "final weights: [-2.82715106e-03 -2.72889831e-03 -2.79174943e-03 -3.02699634e-03\n",
      " -3.98856916e-04  3.76147586e-03 -3.07650701e-03 -1.94716016e-04\n",
      "  2.77559822e-03  3.66834718e-04 -2.79954994e-03 -3.19807182e-03\n",
      "  2.81845044e-03 -3.19571453e-03  2.82879333e-03 -2.75370411e-03\n",
      " -2.78970341e-03 -2.79194520e-03 -2.98053441e-03  3.63145606e-03\n",
      " -7.39423079e-01 -2.31458010e+00 -3.20768464e-01 -5.74317340e-01]\n",
      "0.5299090389540135\n",
      "0 0.14281388729812372\n",
      "1 0.17688552177894126\n",
      "2 0.15303652496824394\n",
      "3 0.17481266424474637\n",
      "4 0.15877827429351313\n",
      "final weights: [-1.47326589e-02  1.24167897e-02 -1.46008281e-02  1.57718398e-02\n",
      " -1.58417045e-02 -1.44737648e-02 -4.29213939e-03  8.08447905e-04\n",
      "  3.70946392e-03 -5.08284437e-03 -1.40980927e-02  1.44703222e-02\n",
      "  1.40979289e-02  1.49160725e-02  1.33729173e-02  1.47816227e-02\n",
      "  1.48710725e-02 -1.45918924e-02  1.57985815e-02  1.57338629e-02\n",
      " -9.69131446e-01  1.08560661e+00  1.21459829e+00  2.03668284e-01]\n",
      "4.292380858800105\n",
      "0 1.0096461978096851\n",
      "1 0.9552286645894247\n",
      "2 0.8019359850036891\n",
      "3 0.5629805431139158\n",
      "4 0.9310291635008328\n",
      "final weights: [-0.03118792 -0.03300947 -0.03187615 -0.00604355  0.03290341 -0.0354906\n",
      " -0.01752605  0.03061011 -0.03088537  0.00247286 -0.03193503  0.02761714\n",
      " -0.02346176 -0.02893873 -0.02681123  0.0309554   0.03098681 -0.03212993\n",
      " -0.00963826  0.03445689  0.24015424 -0.06076093  0.2249941  -0.91511055]\n",
      "0.7335243187385714\n",
      "0 0.14346750880070674\n",
      "1 0.15274863604256037\n",
      "2 0.06956755184610841\n",
      "3 0.12537258281042737\n",
      "4 0.2247186256179484\n",
      "final weights: [-0.13799881  0.08483225  0.12637388 -0.15021074 -0.01989443  0.17409546\n",
      "  0.03442573  0.04304205 -0.04114887  0.01197254 -0.11819472 -0.08703548\n",
      "  0.16285876  0.1424184  -0.1732432  -0.16380285 -0.13810865  0.13573437\n",
      " -0.14835103  0.17280857  1.59398087 -0.39836596 -1.3839267   2.51419167]\n",
      "3.039173047735694\n",
      "0 0.8893059610082883\n",
      "1 1.1395180103799292\n",
      "2 1.3034651370826158\n",
      "3 0.5337308568207397\n",
      "4 1.4196242741184504\n",
      "final weights: [ 2.35786337e-03 -3.32908881e-03 -2.40000863e-03  3.06830772e-03\n",
      "  2.99959812e-03  3.66649337e-03 -2.88519963e-03 -7.14021206e-04\n",
      "  3.00425695e-03  3.20747591e-03  2.21901790e-03  2.97407728e-03\n",
      " -2.53848450e-03  3.06850352e-03  2.61384943e-03 -2.30187553e-03\n",
      " -2.38925739e-03 -2.39891212e-03  3.05107638e-03 -3.80312556e-03\n",
      " -8.75835133e-01 -1.64920413e+00 -4.67493881e-01  1.66627436e+00]\n",
      "0.4244614795366375\n",
      "0 0.21015013854824424\n",
      "1 0.1978503848858275\n",
      "2 0.34116846559527764\n",
      "3 0.19341566485765438\n",
      "4 0.15827928202635155\n",
      "final weights: [-0.01986533 -0.00226207 -0.02014598  0.01640925  0.00771873 -0.01702492\n",
      "  0.01390369 -0.01573166 -0.01622685  0.0151866  -0.02064865  0.01563677\n",
      " -0.01609678  0.01564278 -0.01608868  0.01975894  0.01983142 -0.02012836\n",
      "  0.01632151  0.01802643 -0.60510591  1.85308338  1.47019323  1.48628215]\n",
      "3.7176831584855785\n",
      "0 0.17217710098864217\n",
      "1 0.16633885837488038\n",
      "2 0.17703715001959475\n",
      "3 0.19330633484492094\n",
      "4 0.1827590148069488\n",
      "final weights: [ 0.02667697 -0.0333162  -0.028446    0.01413405 -0.034372    0.03230241\n",
      " -0.03005231 -0.01044462  0.01085686  0.01565772  0.02765623 -0.02415538\n",
      " -0.01549301 -0.02526783 -0.0317804   0.0266078   0.0276724  -0.0284234\n",
      "  0.01305168  0.03511252  2.12204785 -1.41128919 -1.09288962  1.80451849]\n",
      "3.269012480772792\n",
      "0 0.5463737085826309\n",
      "1 0.8041908159005573\n",
      "2 0.5430594875283493\n",
      "3 0.36645975157825156\n",
      "4 0.25825643773005164\n",
      "final weights: [-0.11558599  0.17148941  0.24268968 -0.01923326  0.13643602  0.19444021\n",
      " -0.14557101 -0.03062447  0.16048361 -0.0149439  -0.05908327  0.16085338\n",
      " -0.14563977  0.15806901 -0.1362296   0.11981911  0.2049611   0.24230632\n",
      " -0.06490402 -0.15706392 -0.8521319  -0.95405866  0.45166947  1.55649086]\n",
      "2.3325307588292663\n",
      "0 0.5207922639658328\n",
      "1 0.4734064702483873\n",
      "2 0.4169243585318447\n",
      "3 0.6395288301439807\n",
      "4 0.5758326392885152\n",
      "final weights: [ 2.66447425e-03 -3.25808263e-03 -3.16043040e-03 -2.86548090e-03\n",
      "  3.11716566e-03 -3.41598513e-03 -2.84619667e-03 -5.82673136e-04\n",
      " -2.93889584e-03  1.20511073e-03  3.99334364e-03 -3.02951979e-03\n",
      "  3.58560898e-03 -3.04706117e-03 -3.75652958e-03  3.08847762e-03\n",
      "  3.11751635e-03 -3.16064591e-03 -2.84907791e-03 -3.47883512e-03\n",
      "  9.43947549e-01 -2.33743749e+00  1.40268273e+00  7.22509376e-02]\n",
      "0.7773558694657754\n",
      "0 2.3373663322548546\n",
      "1 2.1468320755843453\n",
      "2 2.3362729103192335\n",
      "3 2.2278456808018356\n",
      "4 1.827781783016177\n",
      "final weights: [-0.01393616 -0.01602384 -0.01434497  0.01860465  0.01673331 -0.01778368\n",
      "  0.01488571  0.015055    0.01597388 -0.01574361 -0.01438878 -0.01353943\n",
      "  0.0136338  -0.013316    0.01450912  0.01399023  0.01372606 -0.01430036\n",
      "  0.01879644  0.01740921  2.10657027 -1.93653918 -2.34166601 -1.9067536 ]\n",
      "0.24049454155930727\n",
      "0 2.1045173881496377\n",
      "1 1.9911403196710655\n",
      "2 2.272398236494974\n",
      "3 2.1732424759052704\n",
      "4 2.2982511244461956\n",
      "final weights: [ 0.0382665   0.03163872  0.03399416 -0.03441716  0.03240085  0.03503376\n",
      " -0.03024064  0.03151087  0.03242328 -0.03177667  0.03532117 -0.02673055\n",
      "  0.03427772 -0.0250847   0.03440729  0.04122972  0.03360285  0.03400982\n",
      " -0.0306311   0.03565245 -1.53936449 -2.03461348 -0.57137376 -1.49093975]\n",
      "0.1905926122859333\n",
      "0 0.2884415009575778\n",
      "1 0.23338508358342192\n",
      "2 0.1920127791148683\n",
      "3 0.16789553420320105\n",
      "4 0.17585627333967585\n",
      "final weights: [ 0.15236043  0.13721533  0.00335141 -0.04229277  0.17111471 -0.09393458\n",
      "  0.02390695 -0.01470802  0.15849352  0.04172033  0.17376255  0.01927918\n",
      "  0.08844898 -0.00599724  0.09297349 -0.16359124 -0.13218907 -0.02652072\n",
      " -0.03637338  0.16081128  0.62765893  0.19845862  1.2325644  -0.62217063]\n",
      "2.531357244547345\n",
      "Max W coef:  13.322890786464272\n",
      "saved weights:  [-4.82830323e-04 -1.54640138e-03  4.60331607e-04  3.43187733e-03\n",
      "  2.08566884e-03 -3.34945539e-03  8.69318183e-04  3.01937090e-03\n",
      "  3.06163798e-03  3.15860240e-03 -5.43815407e-04 -3.20614717e-03\n",
      " -7.90299463e-04  3.29276574e-03  6.38029944e-05 -6.57195855e-04\n",
      " -4.76927360e-04  4.59006397e-04  3.44085987e-03 -2.76262414e-03\n",
      "  9.93067900e-02 -2.21278898e+00  1.30399576e+00  1.62934887e+00]\n"
     ]
    }
   ],
   "source": [
    "qubits = [2,4]\n",
    "batch_sets = [2,4,8,16,32]\n",
    "learning_rates = [0.001,0.005,0.01,0.05]\n",
    "hyperparameters = list(itertools.product(batch_sets,learning_rates))\n",
    "print(hyperparameters)\n",
    "\n",
    "Wmax = 0.0\n",
    "\n",
    "\n",
    "ARCH = ['E1','ZZ','Y','ZZ','Y','ZZ','Y','ZZ','Y','ZZ','Y']\n",
    "for WIRES in qubits:\n",
    "    def temp_circuit(w,features=None): return string2circuit(gate_params=w,features=features,arch_params=ARCH,n_wires=WIRES)\n",
    "    dev = qml.device(\"default.qubit\",wires=WIRES)\n",
    "    outside_circuit = qml.QNode(temp_circuit,device=dev)\n",
    "\n",
    "    #if using zz-layer, use this line\n",
    "    #n_params=np.sum([WIRES for x in ARCH if (x!='E1')])\n",
    "    #if useing CNOT-layer, use this line\n",
    "    n_params=np.sum([WIRES for x in ARCH if (x=='X') or (x=='Y')])\n",
    "    print(n_params)\n",
    "    for idx,sdx in hyperparameters:\n",
    "        wtemp,weights=train_circuit(outside_circuit,n_params,WIRES,X_train,Y_train,s=5,batch_size=idx,rate_type='batch_cost',learning_rate=sdx)\n",
    "        print('final weights:',weights)\n",
    "        print(wtemp)\n",
    "        if wtemp>=Wmax:\n",
    "            Wmax=wtemp\n",
    "            saved_weights = weights\n",
    "            saved_qubits = qdx\n",
    "    print(\"Max W coef: \", Wmax)\n",
    "    print(\"saved weights: \",saved_weights)\n",
    "    print(\"number of qubits: \",saved_qubits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sudden-reward",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 0.1\n"
     ]
    }
   ],
   "source": [
    "for idx, sdx in list(itertools.product([6],[0.1])):\n",
    "    print(idx,sdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detected-eagle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
