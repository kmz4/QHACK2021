{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "actual-customs",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contrary-uruguay",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "Here, we briefly demo the parallel version of QOSE. This code was run succesfully on a ml.m5.24xlarge AWS instance with 96 CPUs, allow for massive parallel evaluation of all proposed circuit architectures in the leaves of the search tree. We used the MPI4Py Message Passing Interface (MPI) to dynamically spawn indivual processes for all leaves in the tree. Since training circuits takes up most of the time, the overhead due to messages being send or blocking due to processes not finishing at the same time is neglible compared to the speed up we can obtain with this approach. Although not fully stable, this version of the code was able to run about 3300 variational circuits for 20 steps (a depth 10 tree with prune rate 0.15) in about three. In additionan, each individual circuit was trained for 3 possible batch sizes and 3 possible learning rates, bringing the total of trained architectures to about $9\\times 3300 \\approx 30000$.\n",
    "\n",
    "In order to run this code, we expect the `qose` package to be installed succesfully with \n",
    "```\n",
    "python setup.py install\n",
    "```\n",
    "Let's get started. Open up `driver_code_par.py`. In this file, we first import all the relevant packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "medical-memphis",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from qose.subarchitecture_tree_search_par import run_tree_architecture_search\n",
    "import pennylane as qml\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-chess",
   "metadata": {},
   "source": [
    "Next, we first create a unique directory to store the data of our tree search algorithm. This will allow us to inspect the tree later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "narrative-twist",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EXPERIMENT_NAME = 'demo'\n",
    "\n",
    "# Create a directory to store the data\n",
    "if not os.path.exists('data'):\n",
    "    os.mkdir('data/')\n",
    "\n",
    "data_path = f'data/{EXPERIMENT_NAME}'\n",
    "if not os.path.exists(data_path):\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-portable",
   "metadata": {},
   "source": [
    "To configure the tree search, we create a configuration file that will be passed to the algorithm. Please refer to the docs of `qose.subarchitecture_tree_search_par` for more detailed information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "completed-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'nqubits': 3,  # number of qubits in the circuit\n",
    "          'embedding': 'E1', # what embedding to use\n",
    "          'min_tree_depth': 4,  # minimum depth before we start pruning\n",
    "          'max_tree_depth': 10,  # maximum circuit depth\n",
    "          'prune_rate': 0.15, # rate with which to prune leaves\n",
    "          'prune_step': 3, # prune only if we have reached min_tree_depth and if (depth % prune_step) == 0\n",
    "          'plot_trees': False, # whether or not to plot trees during algorithm (graphviz required!)\n",
    "          'data_set': 'moons',  # the data set\n",
    "          'nsteps': 20, # number of steps for variational training\n",
    "          'optim': qml.AdamOptimizer, # pennylane optimizer\n",
    "          'batch_sizes': [8, 16, 32, 64], # batch sizes to check for each circuit we construct.\n",
    "          'n_samples': 1500, # number of samples of the data\n",
    "          'learning_rates': [0.001, 0.005, 0.01], # learning rates to check for each circuit we construct.\n",
    "          'save_frequency': 1, # how often save the tree to pickle, 1 means always, 0 never\n",
    "          'save_path': data_path, # where to save the data\n",
    "          'nprocesses': 5, # number of MPI processes spawned, controls the degree of parallelization.\n",
    "          'save_timing': False, # store the time how long each circuit tranining takes\n",
    "          'circuit_type': 'schuld', # specify the list of possible layers (see docs for more info)\n",
    "          'Tmax': [100, 100, 100], # parameters for w-cost function\n",
    "          'inf_time': 'numcnots', # how to determine circuit time, can be with CNOT number of clock time.\n",
    "          'fill': 'redundant',  # embedding parameter \n",
    "          'rate_type': 'accuracy',  # how to evaluate error rate for w-cost\n",
    "          'readout_layer': 'one_hot',  # how to do classiciation.\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-pizza",
   "metadata": {},
   "source": [
    "Then, we save the config so we can remember the details of our experiment,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "given-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_path + '/config.pickle', 'wb') as f:\n",
    "    pickle.dump(config, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-sewing",
   "metadata": {},
   "source": [
    "Finally, we call the function we imported earlier, passing the config and a string specifying if we want to calculate gradients locally, or remotely using the AWS braket SV1 simulator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-lotus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depth = 1\n",
      "current graph:  [('ROOT', {'W': 0.0}), ('E1', {'W': 1.0})]\n",
      "Depth = 2\n",
      "Current best architecture:  E1\n",
      "max W: 1.0\n",
      "Sending chunks: ['E1:ZZ', 'E1:X', 'E1:Y', 'E1:Z']\n"
     ]
    }
   ],
   "source": [
    "run_tree_architecture_search(config, 'local') # if set to 'remote', make sure that S3 buckets and credentials are configured correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-roads",
   "metadata": {},
   "source": [
    "Since we are using MPI4py, we need launch with a special script, otherwise it will do nothing:\n",
    "Open a terminal in the location of the `demo_qose_par.py` file and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mpiexec -n 1 python drive_code_par.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instrumental-resort",
   "metadata": {},
   "source": [
    "If succesful, the master process the controls the logic of the tree should start sending circuit architectures to dynamically spawned subprocesses, \n",
    "and print something like"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "radio-brave",
   "metadata": {},
   "source": [
    "```basgh\n",
    "Depth = 1\n",
    "current graph:  [('ROOT', {'W': 0.0}), ('E1', {'W': 1.0})]\n",
    "Depth = 2\n",
    "Current best architecture:  E1\n",
    "max W: 1.0\n",
    "Sending chunks: ['E1:ZZ', 'E1:X', 'E1:Y', 'E1:Z']\n",
    "CPU 0 doing E1:ZZ\n",
    "CPU 2 doing E1:Y\n",
    "CPU 1 doing E1:X\n",
    "CPU 3 doing E1:Z\n",
    "Depth = 3\n",
    "Current best architecture:  E1:Y\n",
    "max W: 1.8187496589844392\n",
    "Sending chunks: ['E1:ZZ:X', 'E1:ZZ:Y', 'E1:ZZ:Z', 'E1:X:ZZ', 'E1:X:Y', 'E1:X:Z', 'E1:Y:ZZ', 'E1:Y:X', 'E1:Y:Z', 'E1:Z:ZZ', 'E1:Z:X', 'E1:Z:Y']\n",
    "CPU 0 doing E1:ZZ:X\n",
    "CPU 1 doing E1:ZZ:Y\n",
    "CPU 3 doing E1:X:ZZ\n",
    "CPU 4 doing E1:X:Y\n",
    "CPU 8 doing E1:Y:Z\n",
    "CPU 9 doing E1:Z:ZZ\n",
    "CPU 11 doing E1:Z:Y\n",
    "CPU 7 doing E1:Y:X\n",
    "CPU 2 doing E1:ZZ:Z\n",
    "CPU 6 doing E1:Y:ZZ\n",
    "CPU 10 doing E1:Z:X\n",
    "CPU 5 doing E1:X:Z\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-adult",
   "metadata": {},
   "outputs": [],
   "source": [
    "Indicating that the parallel execution is working as intented."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
